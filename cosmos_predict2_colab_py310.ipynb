{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosmos Predict2 on Google Colab (Python 3.10 Required)\n",
    "\n",
    "This notebook runs Cosmos Predict2 on Google Colab.\n",
    "\n",
    "**‚ö†Ô∏è IMPORTANT**: Cosmos-Predict2 requires Python 3.10, but Colab now defaults to Python 3.12.\n",
    "\n",
    "## How to Use Python 3.10 Runtime:\n",
    "\n",
    "### Option 1: Use Fallback Runtime (Recommended)\n",
    "1. Go to **Runtime ‚Üí Change runtime type**\n",
    "2. Under **Runtime version**, select **Use fallback runtime version**\n",
    "3. Select **GPU** (A100, V100, or T4)\n",
    "4. Click **Save**\n",
    "5. The runtime will restart with Python 3.10\n",
    "\n",
    "### Option 2: Use Direct URL\n",
    "Use this URL to create a new notebook with Python 3.10:\n",
    "```\n",
    "https://colab.research.google.com/#create=true&kernelspec=python3.10\n",
    "```\n",
    "\n",
    "After setting up Python 3.10, run the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Verify Python 3.10\n",
    "\n",
    "**Run this first to check your Python version!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Check Python version\n",
    "python_version = sys.version_info\n",
    "print(\"=\"*60)\n",
    "print(f\"üêç Current Python version: {python_version.major}.{python_version.minor}.{python_version.micro}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if python_version.major == 3 and python_version.minor == 10:\n",
    "    print(\"‚úÖ Python 3.10 detected - Compatible with Cosmos-Predict2!\")\n",
    "    print(\"\\nüëç You can proceed with the installation.\")\n",
    "elif python_version.major == 3 and python_version.minor in [11, 12]:\n",
    "    print(f\"\\n‚ùå Python {python_version.major}.{python_version.minor} detected\")\n",
    "    print(\"\\n‚ö†Ô∏è Cosmos-Predict2 requires Python 3.10 due to flash-attn dependencies.\")\n",
    "    print(\"\\nüìù TO FIX THIS:\")\n",
    "    print(\"   1. Click: Runtime ‚Üí Change runtime type\")\n",
    "    print(\"   2. Enable: 'Use fallback runtime version'\")\n",
    "    print(\"   3. Select: GPU (A100, V100, or T4)\")\n",
    "    print(\"   4. Click: Save\")\n",
    "    print(\"   5. The runtime will restart with Python 3.10\")\n",
    "    print(\"\\n   Then run this cell again to verify.\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    raise RuntimeError(\n",
    "        \"Incompatible Python version. Please switch to Python 3.10 using fallback runtime.\"\n",
    "    )\n",
    "else:\n",
    "    print(f\"‚ùå Unexpected Python version: {python_version.major}.{python_version.minor}\")\n",
    "    raise RuntimeError(\"Unsupported Python version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Check GPU\n",
    "\n",
    "Verify GPU is available and has sufficient memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(\"üñ•Ô∏è GPU Check:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    \n",
    "    print(f\"‚úÖ GPU Found: {gpu_name}\")\n",
    "    print(f\"   Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"   PyTorch Version: {torch.__version__}\")\n",
    "    \n",
    "    # GPU recommendations\n",
    "    if \"A100\" in gpu_name:\n",
    "        print(\"\\nüöÄ Excellent! A100 can run the largest models.\")\n",
    "    elif \"V100\" in gpu_name:\n",
    "        print(\"\\n‚úÖ Good! V100 can run medium-sized models.\")\n",
    "    elif \"T4\" in gpu_name:\n",
    "        print(\"\\n‚ö†Ô∏è T4 detected - will use smaller models for better performance.\")\n",
    "    else:\n",
    "        print(f\"\\nüìù {gpu_name} detected - will auto-configure for best performance.\")\n",
    "else:\n",
    "    print(\"‚ùå No GPU detected!\")\n",
    "    print(\"\\nüìù TO FIX THIS:\")\n",
    "    print(\"   1. Go to: Runtime ‚Üí Change runtime type\")\n",
    "    print(\"   2. Set Hardware accelerator to: GPU\")\n",
    "    print(\"   3. GPU type: A100, V100, or T4\")\n",
    "    print(\"   4. Enable: 'Use fallback runtime version' (for Python 3.10)\")\n",
    "    print(\"   5. Click: Save\")\n",
    "    raise RuntimeError(\"GPU required for Cosmos-Predict2\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Install Dependencies\n",
    "\n",
    "Install Cosmos-Predict2 and required packages. We'll use `uv` for faster installation if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv for faster package installation (optional but recommended)\n",
    "print(\"üì¶ Setting up package manager...\\n\")\n",
    "\n",
    "try:\n",
    "    # Try to install uv\n",
    "    !curl -LsSf https://astral.sh/uv/install.sh | sh 2>/dev/null\n",
    "    os.environ['PATH'] = f\"{os.path.expanduser('~/.local/bin')}:{os.environ['PATH']}\"\n",
    "    \n",
    "    # Check if uv is available\n",
    "    !uv --version\n",
    "    USE_UV = True\n",
    "    print(\"\\n‚úÖ Will use 'uv' for faster installation (3-10x faster than pip)\")\n",
    "except:\n",
    "    USE_UV = False\n",
    "    print(\"‚ÑπÔ∏è Will use standard pip for installation\")\n",
    "\n",
    "print(\"\\nThis installation will take 2-5 minutes...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture install_output\n",
    "\n",
    "# Install Cosmos-Predict2 and dependencies\n",
    "if USE_UV:\n",
    "    # Using uv (faster)\n",
    "    !uv pip install --system --upgrade pip setuptools wheel\n",
    "    !uv pip install --system torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    !uv pip install --system \"cosmos-predict2[cu126]\" --extra-index-url https://nvidia-cosmos.github.io/cosmos-dependencies/cu126_torch260/simple\n",
    "    !uv pip install --system transformers accelerate bitsandbytes\n",
    "    !uv pip install --system decord einops \"imageio[ffmpeg]\" opencv-python-headless pillow\n",
    "else:\n",
    "    # Using pip (standard)\n",
    "    !pip install --upgrade pip setuptools wheel\n",
    "    !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    !pip install \"cosmos-predict2[cu126]\" --extra-index-url https://nvidia-cosmos.github.io/cosmos-dependencies/cu126_torch260/simple\n",
    "    !pip install transformers accelerate bitsandbytes\n",
    "    !pip install decord einops \"imageio[ffmpeg]\" opencv-python-headless pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check installation status\n",
    "print(\"üì¶ Installation Status:\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check for errors in installation\n",
    "install_text = install_output.stdout.lower()\n",
    "if \"error\" in install_text or \"failed\" in install_text:\n",
    "    print(\"‚ö†Ô∏è Some packages had installation warnings/errors.\")\n",
    "    print(\"   This is often okay. Let's test the import...\\n\")\n",
    "else:\n",
    "    print(\"‚úÖ All packages installed without errors\\n\")\n",
    "\n",
    "# Test Cosmos import\n",
    "try:\n",
    "    from cosmos_predict2.inference import Video2WorldPipeline\n",
    "    print(\"‚úÖ Cosmos-Predict2 imported successfully!\")\n",
    "    print(\"\\nüéâ Installation complete! You're ready to use Cosmos-Predict2.\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import failed: {e}\")\n",
    "    print(\"\\nüìù TO FIX THIS:\")\n",
    "    print(\"   1. Try: Runtime ‚Üí Restart runtime\")\n",
    "    print(\"   2. Run the cells again from the beginning\")\n",
    "    print(\"   3. Make sure you're using Python 3.10 (fallback runtime)\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Mount Google Drive (Optional but Recommended)\n",
    "\n",
    "Save your outputs to Google Drive to prevent data loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Drive\n",
    "mount_drive = True  # Set to False to skip\n",
    "\n",
    "if mount_drive:\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Create output directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    drive_output_dir = f\"/content/drive/MyDrive/cosmos_outputs_{timestamp}\"\n",
    "    os.makedirs(drive_output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"‚úÖ Google Drive mounted\")\n",
    "    print(f\"üìÅ Outputs will be saved to: {drive_output_dir}\")\n",
    "    \n",
    "    # Save session info\n",
    "    with open(f\"{drive_output_dir}/session_info.txt\", \"w\") as f:\n",
    "        f.write(f\"Session started: {datetime.now()}\\n\")\n",
    "        f.write(f\"Python version: {sys.version}\\n\")\n",
    "        f.write(f\"GPU: {gpu_name if 'gpu_name' in locals() else 'Unknown'}\\n\")\n",
    "        f.write(f\"GPU Memory: {gpu_memory if 'gpu_memory' in locals() else 0:.1f} GB\\n\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Google Drive not mounted - outputs will be lost if session disconnects\")\n",
    "    drive_output_dir = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Download Model Checkpoint\n",
    "\n",
    "Download the Cosmos-Predict2 model. Size is auto-selected based on your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "import glob\n",
    "\n",
    "# Auto-select model size based on GPU memory\n",
    "if 'gpu_memory' not in locals():\n",
    "    gpu_memory = 16  # Default assumption\n",
    "\n",
    "if gpu_memory >= 40:  # A100\n",
    "    MODEL_SIZE = \"14B\"\n",
    "    print(\"üöÄ Using Cosmos-14B (largest, best quality)\")\n",
    "elif gpu_memory >= 24:  # A10G, 3090\n",
    "    MODEL_SIZE = \"5B\"\n",
    "    print(\"‚úÖ Using Cosmos-5B (medium, good quality)\")\n",
    "else:  # T4, V100, or less\n",
    "    MODEL_SIZE = \"2B\"\n",
    "    print(\"üì¶ Using Cosmos-2B (small, efficient)\")\n",
    "\n",
    "print(f\"\\nDownloading Cosmos-Predict2-{MODEL_SIZE} checkpoint...\")\n",
    "print(\"This will take 2-5 minutes...\\n\")\n",
    "\n",
    "# Download checkpoint\n",
    "checkpoint_dir = snapshot_download(\n",
    "    repo_id=f\"nvidia/Cosmos-Predict2-{MODEL_SIZE}-Video2World\",\n",
    "    cache_dir=\"/content/cosmos_checkpoints\",\n",
    "    resume_download=True\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Checkpoint downloaded successfully\")\n",
    "\n",
    "# Find model files\n",
    "model_files = glob.glob(f\"{checkpoint_dir}/**/*.pt\", recursive=True)\n",
    "if model_files:\n",
    "    print(f\"Found {len(model_files)} model file(s)\")\n",
    "    model_path = model_files[0]\n",
    "    print(f\"Will use: {os.path.basename(model_path)}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No .pt files found - will use default path\")\n",
    "    model_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Quick Test - Generate a Video\n",
    "\n",
    "Let's do a quick test to generate a video and verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image as PILImage\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "from cosmos_predict2.inference import (\n",
    "    Video2WorldPipeline,\n",
    "    get_cosmos_predict2_video2world_pipeline,\n",
    ")\n",
    "from einops import rearrange\n",
    "import imageio\n",
    "\n",
    "print(\"Loading models...\\n\")\n",
    "\n",
    "# 1. Load T5 text encoder\n",
    "t5_model = \"google/flan-t5-base\" if gpu_memory < 24 else \"google/flan-t5-xl\"\n",
    "print(f\"Loading T5 encoder ({t5_model})...\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(t5_model)\n",
    "text_encoder = T5EncoderModel.from_pretrained(t5_model).half().to(\"cuda\")\n",
    "text_encoder.eval()\n",
    "print(\"‚úÖ T5 loaded\\n\")\n",
    "\n",
    "# 2. Load Cosmos pipeline\n",
    "print(f\"Loading Cosmos-{MODEL_SIZE} pipeline...\")\n",
    "config = get_cosmos_predict2_video2world_pipeline(model_size=MODEL_SIZE)\n",
    "if model_path:\n",
    "    config['dit_checkpoint_path'] = model_path\n",
    "\n",
    "cosmos_pipe = Video2WorldPipeline.from_config(config)\n",
    "cosmos_pipe = cosmos_pipe.to(\"cuda\")\n",
    "cosmos_pipe.eval()\n",
    "print(\"‚úÖ Cosmos pipeline loaded\\n\")\n",
    "\n",
    "print(f\"üíæ GPU memory used: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "print(\"\\nReady to generate!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple test image\n",
    "print(\"Creating test input...\\n\")\n",
    "\n",
    "# Create a simple colored image\n",
    "img = np.ones((720, 1280, 3), dtype=np.uint8) * 100\n",
    "img[200:520, 400:880] = [200, 150, 100]  # Add a colored rectangle\n",
    "test_image = PILImage.fromarray(img)\n",
    "test_image.save(\"test_input.jpg\")\n",
    "\n",
    "# Display the input\n",
    "from IPython.display import display\n",
    "display(test_image)\n",
    "print(\"\\n‚úÖ Test input created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a video\n",
    "print(\"üé¨ Generating video...\\n\")\n",
    "\n",
    "# 1. Encode text prompt\n",
    "prompt = \"A robotic arm moves across the table\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=77, \n",
    "                  padding=\"max_length\", truncation=True).to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(**inputs).last_hidden_state\n",
    "\n",
    "# 2. Prepare input image\n",
    "frames = np.array(test_image)[np.newaxis, ...]\n",
    "frames_tensor = torch.from_numpy(frames).float() / 255.0\n",
    "frames_tensor = rearrange(frames_tensor, \"t h w c -> 1 c t h w\").to(\"cuda\")\n",
    "\n",
    "# 3. Generate video (fewer frames for quick test)\n",
    "num_frames = 16  # Quick test with 16 frames\n",
    "print(f\"Generating {num_frames} frames...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output = cosmos_pipe(\n",
    "            frames_tensor,\n",
    "            text_embeddings,\n",
    "            num_frames=num_frames,\n",
    "            fps=8,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "print(\"‚úÖ Video generated!\\n\")\n",
    "\n",
    "# 4. Save video\n",
    "if isinstance(output, torch.Tensor):\n",
    "    video = output.cpu().numpy()\n",
    "else:\n",
    "    video = output\n",
    "\n",
    "if video.ndim == 5:\n",
    "    video = video[0]\n",
    "if video.shape[0] == 3:\n",
    "    video = np.transpose(video, (1, 2, 3, 0))\n",
    "if video.max() <= 1.0:\n",
    "    video = (video * 255).astype(np.uint8)\n",
    "\n",
    "# Save\n",
    "output_path = \"test_output.mp4\"\n",
    "writer = imageio.get_writer(output_path, fps=8)\n",
    "for frame in video:\n",
    "    writer.append_data(frame)\n",
    "writer.close()\n",
    "\n",
    "print(f\"‚úÖ Saved: {output_path}\")\n",
    "\n",
    "# Save to Drive if mounted\n",
    "if drive_output_dir:\n",
    "    import shutil\n",
    "    drive_path = f\"{drive_output_dir}/test_output.mp4\"\n",
    "    shutil.copy2(output_path, drive_path)\n",
    "    print(f\"‚òÅÔ∏è Backed up to Google Drive\")\n",
    "\n",
    "# Display video\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "with open(output_path, 'rb') as f:\n",
    "    video_data = f.read()\n",
    "encoded = base64.b64encode(video_data).decode('ascii')\n",
    "\n",
    "display(HTML(f'''\n",
    "<video width=\"640\" controls autoplay loop>\n",
    "    <source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\">\n",
    "</video>\n",
    "'''))\n",
    "\n",
    "print(\"\\nüéâ Success! Cosmos-Predict2 is working correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Setup Complete!\n",
    "\n",
    "Cosmos-Predict2 is now ready to use! You can:\n",
    "\n",
    "1. **Generate longer videos**: Increase `num_frames` (16, 61, or 121 depending on GPU)\n",
    "2. **Use different prompts**: Change the text prompt for different motions\n",
    "3. **Use your own images**: Upload your own input images\n",
    "4. **Batch process**: Generate multiple videos with different prompts\n",
    "\n",
    "### Memory Guide:\n",
    "\n",
    "| GPU | Max Frames | Quality |\n",
    "|-----|------------|------|\n",
    "| A100 | 121 | Best |\n",
    "| V100 | 61 | Good |\n",
    "| T4 | 16-30 | OK |\n",
    "\n",
    "### Tips:\n",
    "- Always save to Google Drive to prevent data loss\n",
    "- Clear GPU cache between generations: `torch.cuda.empty_cache()`\n",
    "- Use smaller models if you run out of memory\n",
    "\n",
    "### Troubleshooting:\n",
    "- **Python version error**: Make sure you're using the fallback runtime (Python 3.10)\n",
    "- **Import errors**: Restart runtime and run cells again\n",
    "- **Out of memory**: Reduce `num_frames` or restart runtime to clear memory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}