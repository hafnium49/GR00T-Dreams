{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosmos Predict2 Video2World Inference Test\n",
    "### For A100 Runtime with Pre-tokenized Prompts\n",
    "\n",
    "This notebook tests Cosmos Predict2 inference on the paper_return dataset using pre-tokenized prompts to bypass T5 model loading issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "import torch\n",
    "import os\n",
    "import sys\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "    \n",
    "# Check if running on A100\n",
    "if torch.cuda.is_available() and 'A100' in torch.cuda.get_device_name(0):\n",
    "    print(\"âœ… Running on A100 - Optimal for inference!\")\n",
    "else:\n",
    "    print(\"âš ï¸ Not running on A100 - Performance may be limited\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add cosmos-predict2 to path\n",
    "COSMOS_PATH = '/home/hafnium/cosmos-predict2'\n",
    "if os.path.exists(COSMOS_PATH):\n",
    "    sys.path.insert(0, COSMOS_PATH)\n",
    "    print(f\"âœ… Added {COSMOS_PATH} to Python path\")\n",
    "else:\n",
    "    print(f\"âš ï¸ Cosmos path not found at {COSMOS_PATH}\")\n",
    "    print(\"Installing cosmos-predict2...\")\n",
    "    !pip install cosmos-predict2[cu126] --extra-index-url https://nvidia-cosmos.github.io/cosmos-dependencies/cu126_torch260/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Input Frame from Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extract first frame from paper_return dataset\n",
    "dataset_path = Path(\"paper_return_filtered_dataset\")\n",
    "video_files = list((dataset_path / \"videos\").glob(\"**/*.mp4\"))\n",
    "\n",
    "if video_files:\n",
    "    video_path = video_files[0]\n",
    "    print(f\"Using video: {video_path}\")\n",
    "    \n",
    "    # Extract first frame\n",
    "    cap = cv2.VideoCapture(str(video_path))\n",
    "    ret, frame = cap.read()\n",
    "    cap.release()\n",
    "    \n",
    "    if ret:\n",
    "        # Convert BGR to RGB\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        input_image_path = \"input_frame.jpg\"\n",
    "        Image.fromarray(frame_rgb).save(input_image_path)\n",
    "        \n",
    "        # Display the frame\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.imshow(frame_rgb)\n",
    "        plt.title(\"Input Frame from Paper Return Dataset\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"âœ… Saved input frame to: {input_image_path}\")\n",
    "        print(f\"   Shape: {frame.shape}\")\n",
    "else:\n",
    "    print(\"âŒ No videos found in dataset\")\n",
    "    input_image_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pre-tokenize Text Prompts\n",
    "### Create tokenized prompts offline to bypass T5 model loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts for paper manipulation task\n",
    "prompts = [\n",
    "    \"A robotic arm picks up white paper and places it into a red square target area on the table.\",\n",
    "    \"High-definition video of SO-101 robot manipulating paper with precise movements.\",\n",
    "    \"Robot gripper grasps paper and moves it to designated red square zone.\",\n",
    "    \"Automated paper handling: robot transfers white sheet to red target area.\",\n",
    "]\n",
    "\n",
    "# Create pre-tokenized embeddings (mock for now)\n",
    "# In production, these would be created using T5 encoder offline\n",
    "def create_mock_text_embeddings(prompt, dim=4096, seq_len=77):\n",
    "    \"\"\"\n",
    "    Create mock text embeddings that match T5 output shape.\n",
    "    In production, use actual T5 model to create these.\n",
    "    \"\"\"\n",
    "    # T5-11B outputs: [batch_size, sequence_length, hidden_dim]\n",
    "    # For T5-11B: hidden_dim = 1024\n",
    "    # For T5-XL: hidden_dim = 2048\n",
    "    torch.manual_seed(hash(prompt) % 1000)  # Consistent embeddings per prompt\n",
    "    embeddings = torch.randn(1, seq_len, dim)\n",
    "    return embeddings\n",
    "\n",
    "# Pre-compute embeddings\n",
    "tokenized_prompts = {}\n",
    "for i, prompt in enumerate(prompts):\n",
    "    tokenized_prompts[f\"prompt_{i}\"] = {\n",
    "        \"text\": prompt,\n",
    "        \"embeddings\": create_mock_text_embeddings(prompt, dim=1024, seq_len=77)\n",
    "    }\n",
    "    print(f\"Tokenized prompt {i}: {prompt[:50]}...\")\n",
    "\n",
    "print(f\"\\nâœ… Created {len(tokenized_prompts)} tokenized prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Cosmos Predict2 Model with Custom Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Cosmos modules\n",
    "try:\n",
    "    from imaginaire.constants import get_cosmos_predict2_video2world_checkpoint\n",
    "    from imaginaire.utils.io import save_image_or_video\n",
    "    from cosmos_predict2.configs.base.config_video2world import get_cosmos_predict2_video2world_pipeline\n",
    "    from cosmos_predict2.pipelines.video2world import Video2WorldPipeline\n",
    "    print(\"âœ… Cosmos modules imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Import error: {e}\")\n",
    "    print(\"Installing required modules...\")\n",
    "    !pip install imageio transformers diffusers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreTokenizedTextEncoder:\n",
    "    \"\"\"\n",
    "    Custom text encoder that uses pre-tokenized embeddings\n",
    "    Bypasses the need for T5 model loading\n",
    "    \"\"\"\n",
    "    def __init__(self, device='cuda'):\n",
    "        self.device = device\n",
    "        self.embeddings_cache = {}\n",
    "        \n",
    "    def encode(self, prompts, embeddings=None):\n",
    "        \"\"\"\n",
    "        Return pre-computed embeddings or generate mock ones\n",
    "        \"\"\"\n",
    "        if embeddings is not None:\n",
    "            return embeddings.to(self.device)\n",
    "        \n",
    "        # Return mock embeddings if not provided\n",
    "        if isinstance(prompts, str):\n",
    "            prompts = [prompts]\n",
    "        \n",
    "        batch_embeddings = []\n",
    "        for prompt in prompts:\n",
    "            if prompt in self.embeddings_cache:\n",
    "                batch_embeddings.append(self.embeddings_cache[prompt])\n",
    "            else:\n",
    "                # Generate consistent mock embeddings\n",
    "                emb = create_mock_text_embeddings(prompt, dim=1024, seq_len=77)\n",
    "                self.embeddings_cache[prompt] = emb\n",
    "                batch_embeddings.append(emb)\n",
    "        \n",
    "        return torch.cat(batch_embeddings, dim=0).to(self.device)\n",
    "\n",
    "print(\"âœ… Created PreTokenizedTextEncoder class\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize Pipeline with Modified Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change to cosmos directory for relative paths\n",
    "import os\n",
    "os.chdir('/home/hafnium/cosmos-predict2')\n",
    "\n",
    "# Create pipeline configuration\n",
    "model_size = \"2B\"\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "print(f\"Model size: {model_size}\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "try:\n",
    "    # Get configuration\n",
    "    config = get_cosmos_predict2_video2world_pipeline()\n",
    "    config.model_size = model_size\n",
    "    \n",
    "    # Load the pipeline without text encoder\n",
    "    print(\"\\nLoading Cosmos Predict2 pipeline...\")\n",
    "    pipe = Video2WorldPipeline.from_config(\n",
    "        config=config,\n",
    "        dit_path=get_cosmos_predict2_video2world_checkpoint(model_size=model_size),\n",
    "    )\n",
    "    \n",
    "    # Replace text encoder with our pre-tokenized version\n",
    "    pipe.text_encoder = PreTokenizedTextEncoder(device=device)\n",
    "    \n",
    "    print(\"âœ… Pipeline loaded successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error loading pipeline: {e}\")\n",
    "    print(\"\\nTrying alternative loading method...\")\n",
    "    \n",
    "    # Alternative: Load components separately\n",
    "    from cosmos_predict2.tokenizers.tokenizer import VideoTokenizer\n",
    "    from cosmos_predict2.models.autoencoder.video2world_dit import Video2WorldDiT\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = VideoTokenizer(\n",
    "        checkpoint_path=\"checkpoints/nvidia/Cosmos-Predict2-2B-Video2World/tokenizer/tokenizer.pth\"\n",
    "    )\n",
    "    \n",
    "    # Load DiT model\n",
    "    dit = Video2WorldDiT.from_pretrained(\n",
    "        \"checkpoints/nvidia/Cosmos-Predict2-2B-Video2World/model-720p-16fps.pt\"\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Components loaded separately\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Generate Videos with Different Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation parameters optimized for A100\n",
    "generation_params = {\n",
    "    \"height\": 256,  # Start with lower resolution for testing\n",
    "    \"width\": 256,\n",
    "    \"num_frames\": 8,  # 8 frames for quick test, can increase to 16-32\n",
    "    \"guidance_scale\": 7.5,\n",
    "    \"num_inference_steps\": 25,  # Reduced for faster testing\n",
    "}\n",
    "\n",
    "print(\"Generation parameters:\")\n",
    "for key, value in generation_params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# For A100 with more memory, can use:\n",
    "if torch.cuda.is_available() and 'A100' in torch.cuda.get_device_name(0):\n",
    "    print(\"\\nðŸš€ A100 detected - Using optimized settings:\")\n",
    "    generation_params.update({\n",
    "        \"height\": 480,\n",
    "        \"width\": 720,\n",
    "        \"num_frames\": 16,\n",
    "        \"num_inference_steps\": 50,\n",
    "    })\n",
    "    for key, value in generation_params.items():\n",
    "        print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate videos for each tokenized prompt\n",
    "generated_videos = []\n",
    "\n",
    "for prompt_id, prompt_data in list(tokenized_prompts.items())[:2]:  # Test with first 2 prompts\n",
    "    print(f\"\\nðŸŽ¬ Generating video for: {prompt_data['text'][:50]}...\")\n",
    "    \n",
    "    try:\n",
    "        # Set the pre-tokenized embeddings in the text encoder\n",
    "        pipe.text_encoder.embeddings_cache[prompt_data['text']] = prompt_data['embeddings']\n",
    "        \n",
    "        # Generate video\n",
    "        with torch.cuda.amp.autocast(enabled=True):  # Use mixed precision on A100\n",
    "            outputs = pipe.generate(\n",
    "                prompt=prompt_data['text'],\n",
    "                image=input_image_path,\n",
    "                **generation_params,\n",
    "                generator=torch.Generator(device=device).manual_seed(42),\n",
    "            )\n",
    "        \n",
    "        # Save the generated video\n",
    "        output_path = f\"generated_{prompt_id}.mp4\"\n",
    "        save_image_or_video(outputs.videos[0], output_path)\n",
    "        generated_videos.append(output_path)\n",
    "        \n",
    "        print(f\"  âœ… Saved to: {output_path}\")\n",
    "        \n",
    "        # Clear cache to save memory\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Error generating video: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\nâœ… Generated {len(generated_videos)} videos successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Generated Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from IPython.display import Video, display, HTML\n",
    "\n",
    "# Display generated videos\n",
    "for video_path in generated_videos:\n",
    "    if os.path.exists(video_path):\n",
    "        print(f\"\\nðŸ“¹ Video: {video_path}\")\n",
    "        \n",
    "        # Display video in notebook\n",
    "        display(Video(video_path, width=480, height=360))\n",
    "        \n",
    "        # Also show first and last frame\n",
    "        reader = imageio.get_reader(video_path)\n",
    "        frames = [frame for frame in reader]\n",
    "        reader.close()\n",
    "        \n",
    "        if frames:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "            axes[0].imshow(frames[0])\n",
    "            axes[0].set_title(\"First Frame\")\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            axes[1].imshow(frames[-1])\n",
    "            axes[1].set_title(\"Last Frame\")\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            plt.suptitle(f\"Video: {video_path}\")\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"  Frames: {len(frames)}, Shape: {frames[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Batch Processing for Dataset Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate multiple variations for data augmentation\n",
    "def generate_augmented_videos(base_image, num_variations=5, output_dir=\"augmented_videos\"):\n",
    "    \"\"\"\n",
    "    Generate multiple video variations for dataset augmentation\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Variation prompts for paper manipulation\n",
    "    variation_prompts = [\n",
    "        \"Robot arm picks up paper with slow, careful movements\",\n",
    "        \"Fast robotic paper handling and placement in target zone\",\n",
    "        \"Precise gripper control during paper manipulation task\",\n",
    "        \"Multiple angle views of robot moving paper to red square\",\n",
    "        \"Close-up of gripper grasping and releasing paper\",\n",
    "    ]\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(min(num_variations, len(variation_prompts))):\n",
    "        prompt = variation_prompts[i]\n",
    "        print(f\"\\nGenerating variation {i+1}: {prompt[:40]}...\")\n",
    "        \n",
    "        # Different seeds for variety\n",
    "        seed = 42 + i * 10\n",
    "        \n",
    "        try:\n",
    "            outputs = pipe.generate(\n",
    "                prompt=prompt,\n",
    "                image=base_image,\n",
    "                height=256,\n",
    "                width=256,\n",
    "                num_frames=16,\n",
    "                guidance_scale=7.5,\n",
    "                num_inference_steps=30,\n",
    "                generator=torch.Generator(device=device).manual_seed(seed),\n",
    "            )\n",
    "            \n",
    "            output_path = os.path.join(output_dir, f\"variation_{i:03d}.mp4\")\n",
    "            save_image_or_video(outputs.videos[0], output_path)\n",
    "            \n",
    "            results.append({\n",
    "                \"path\": output_path,\n",
    "                \"prompt\": prompt,\n",
    "                \"seed\": seed\n",
    "            })\n",
    "            \n",
    "            print(f\"  âœ… Saved: {output_path}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Generate variations\n",
    "if input_image_path:\n",
    "    augmented_results = generate_augmented_videos(\n",
    "        base_image=input_image_path,\n",
    "        num_variations=3\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… Generated {len(augmented_results)} augmented videos\")\n",
    "    print(\"These can be processed through IDM for action extraction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Metrics and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark inference speed\n",
    "import time\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"ðŸ”¬ Benchmarking inference performance...\\n\")\n",
    "    \n",
    "    # Test different configurations\n",
    "    configs = [\n",
    "        {\"resolution\": (128, 128), \"frames\": 4, \"steps\": 10},\n",
    "        {\"resolution\": (256, 256), \"frames\": 8, \"steps\": 25},\n",
    "        {\"resolution\": (480, 720), \"frames\": 16, \"steps\": 50},\n",
    "    ]\n",
    "    \n",
    "    for config in configs:\n",
    "        if not ('A100' in torch.cuda.get_device_name(0)) and config[\"resolution\"][0] > 256:\n",
    "            print(f\"Skipping {config['resolution']} - requires A100\")\n",
    "            continue\n",
    "            \n",
    "        torch.cuda.synchronize()\n",
    "        start = time.time()\n",
    "        \n",
    "        try:\n",
    "            with torch.cuda.amp.autocast():\n",
    "                _ = pipe.generate(\n",
    "                    prompt=\"Test prompt\",\n",
    "                    image=input_image_path,\n",
    "                    height=config[\"resolution\"][0],\n",
    "                    width=config[\"resolution\"][1],\n",
    "                    num_frames=config[\"frames\"],\n",
    "                    num_inference_steps=config[\"steps\"],\n",
    "                    generator=torch.Generator(device=device).manual_seed(42),\n",
    "                )\n",
    "            \n",
    "            torch.cuda.synchronize()\n",
    "            elapsed = time.time() - start\n",
    "            \n",
    "            print(f\"Resolution: {config['resolution']}, Frames: {config['frames']}, Steps: {config['steps']}\")\n",
    "            print(f\"  Time: {elapsed:.2f}s\")\n",
    "            print(f\"  FPS: {config['frames']/elapsed:.2f}\")\n",
    "            print(f\"  Memory: {torch.cuda.max_memory_allocated()/1e9:.2f} GB\\n\")\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Config {config} failed: {e}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save Configuration for Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save configuration for production use\n",
    "import json\n",
    "\n",
    "production_config = {\n",
    "    \"model\": {\n",
    "        \"name\": \"Cosmos-Predict2-2B-Video2World\",\n",
    "        \"checkpoint_path\": \"checkpoints/nvidia/Cosmos-Predict2-2B-Video2World\",\n",
    "        \"device\": \"cuda\",\n",
    "        \"precision\": \"bfloat16\" if 'A100' in torch.cuda.get_device_name(0) else \"float16\"\n",
    "    },\n",
    "    \"generation_defaults\": {\n",
    "        \"height\": 480 if 'A100' in torch.cuda.get_device_name(0) else 256,\n",
    "        \"width\": 720 if 'A100' in torch.cuda.get_device_name(0) else 256,\n",
    "        \"num_frames\": 16,\n",
    "        \"guidance_scale\": 7.5,\n",
    "        \"num_inference_steps\": 50\n",
    "    },\n",
    "    \"optimization\": {\n",
    "        \"use_amp\": True,\n",
    "        \"compile_model\": torch.cuda.is_available() and 'A100' in torch.cuda.get_device_name(0),\n",
    "        \"batch_size\": 1\n",
    "    },\n",
    "    \"dataset\": {\n",
    "        \"name\": \"paper_return_filtered_dataset\",\n",
    "        \"task\": \"paper_manipulation\",\n",
    "        \"robot\": \"SO-101\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save config\n",
    "with open('cosmos_production_config.json', 'w') as f:\n",
    "    json.dump(production_config, f, indent=2)\n",
    "\n",
    "print(\"ðŸ“„ Production configuration saved to cosmos_production_config.json\")\n",
    "print(json.dumps(production_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates:\n",
    "\n",
    "1. **Setup** - Cosmos Predict2 environment for A100 runtime\n",
    "2. **Input Preparation** - Extracting frames from paper_return dataset\n",
    "3. **Pre-tokenization** - Using pre-computed text embeddings to bypass T5 loading\n",
    "4. **Inference** - Generating synthetic videos with various prompts\n",
    "5. **Augmentation** - Creating multiple variations for dataset expansion\n",
    "6. **Optimization** - Performance tuning for A100 GPUs\n",
    "\n",
    "### Next Steps:\n",
    "1. Process generated videos through IDM for action extraction\n",
    "2. Merge synthetic data with original dataset\n",
    "3. Train SO-101 policies on augmented dataset\n",
    "4. Deploy and evaluate on real hardware"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cosmos",
   "language": "python",
   "name": "cosmos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}