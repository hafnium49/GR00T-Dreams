{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosmos Predict2 Full Pipeline on A100\n",
    "\n",
    "This notebook runs both T5 encoding and Cosmos Predict2 inference on a single A100 GPU.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with A100 runtime\n",
    "- 40GB GPU memory\n",
    "\n",
    "**Note:** Make sure to select `Runtime > Change runtime type > A100 GPU` before running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Installation Setup\n",
    "\n",
    "Choose installation method: GitHub source (latest features) or PyPI (stable release)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set installation method\n",
    "USE_GITHUB = True  # Set to True for latest features from GitHub, False for stable PyPI release\n",
    "\n",
    "if USE_GITHUB:\n",
    "    print(\"📦 Installing Cosmos Predict2 from GitHub source...\")\n",
    "else:\n",
    "    print(\"📦 Installing Cosmos Predict2 from PyPI...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install from GitHub Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "if USE_GITHUB:\n",
    "    # Clone the repository\n",
    "    !git clone https://github.com/nvidia-cosmos/cosmos-predict2.git /content/cosmos-predict2\n",
    "    \n",
    "    # Change to the repo directory\n",
    "    import os\n",
    "    os.chdir('/content/cosmos-predict2')\n",
    "    \n",
    "    # Install PyTorch with CUDA support\n",
    "    !pip install -q --upgrade pip\n",
    "    !pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    \n",
    "    # Install cosmos-predict2 from source with CUDA support\n",
    "    !pip install -q -e \".[cu126]\" --extra-index-url https://nvidia-cosmos.github.io/cosmos-dependencies/cu126_torch260/simple\n",
    "    \n",
    "    # Add to Python path\n",
    "    import sys\n",
    "    sys.path.insert(0, '/content/cosmos-predict2')\n",
    "    \n",
    "    print(\"✅ Installed from GitHub source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install from PyPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "if not USE_GITHUB:\n",
    "    # Install PyTorch with CUDA support\n",
    "    !pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    \n",
    "    # Install Cosmos Predict2 from PyPI\n",
    "    !pip install -q \"cosmos-predict2[cu126]\" --extra-index-url https://nvidia-cosmos.github.io/cosmos-dependencies/cu126_torch260/simple\n",
    "    \n",
    "    print(\"✅ Installed from PyPI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Additional Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install other required dependencies\n",
    "!pip install -q transformers accelerate bitsandbytes\n",
    "!pip install -q decord einops imageio[ffmpeg]\n",
    "!pip install -q opencv-python-headless pillow\n",
    "\n",
    "print(\"✅ Additional dependencies installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Verify Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installations and setup paths\n",
    "import pkg_resources\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Add cosmos-predict2 to path if using GitHub installation\n",
    "if os.path.exists('/content/cosmos-predict2'):\n",
    "    sys.path.insert(0, '/content/cosmos-predict2')\n",
    "    COSMOS_PATH = '/content/cosmos-predict2'\n",
    "    print(f\"✅ Using Cosmos Predict2 from GitHub: {COSMOS_PATH}\")\n",
    "else:\n",
    "    COSMOS_PATH = None\n",
    "    print(\"✅ Using Cosmos Predict2 from pip installation\")\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"🖥️ GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n",
    "else:\n",
    "    print(\"❌ No GPU detected!\")\n",
    "\n",
    "# Test import\n",
    "try:\n",
    "    from cosmos_predict2.inference import Video2WorldPipeline\n",
    "    print(\"✅ Cosmos Predict2 imports working correctly\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Import error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mount Google Drive (Optional but Recommended)\n",
    "\n",
    "Mount your Google Drive to auto-save outputs and prevent data loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for automatic saving\n",
    "mount_drive = True  # Set to True to auto-save outputs to Google Drive\n",
    "\n",
    "if mount_drive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"✅ Google Drive mounted at /content/drive\")\n",
    "    \n",
    "    # Create output directory in Drive\n",
    "    from datetime import datetime\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    drive_output_dir = f\"/content/drive/MyDrive/cosmos_outputs_{timestamp}\"\n",
    "    os.makedirs(drive_output_dir, exist_ok=True)\n",
    "    print(f\"📁 Output directory created: {drive_output_dir}\")\n",
    "    print(\"💾 All outputs will be automatically saved to Google Drive\")\n",
    "else:\n",
    "    print(\"⚠️ WARNING: Google Drive not mounted - outputs may be lost if runtime disconnects!\")\n",
    "    print(\"   Set mount_drive=True to enable automatic saving\")\n",
    "    drive_output_dir = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Download Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select model size based on GPU\n",
    "if gpu_memory >= 40:  # A100\n",
    "    MODEL_SIZE = \"14B\"  # Can use largest model\n",
    "elif gpu_memory >= 16:  # T4 or similar\n",
    "    MODEL_SIZE = \"5B\"  # Medium model\n",
    "else:\n",
    "    MODEL_SIZE = \"2B\"  # Smallest model\n",
    "\n",
    "print(f\"🤖 Selected Cosmos Predict2-{MODEL_SIZE} based on {gpu_memory:.1f}GB GPU\")\n",
    "print(\"Downloading checkpoint (this may take a few minutes)...\")\n",
    "\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Download checkpoint\n",
    "checkpoint_base_dir = \"/content/cosmos_checkpoints\"\n",
    "checkpoint_dir = snapshot_download(\n",
    "    repo_id=f\"nvidia/Cosmos-Predict2-{MODEL_SIZE}-Video2World\",\n",
    "    cache_dir=checkpoint_base_dir,\n",
    "    resume_download=True\n",
    ")\n",
    "\n",
    "print(f\"✅ Checkpoint downloaded to: {checkpoint_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize T5 Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "import torch\n",
    "\n",
    "class OptimizedT5Encoder:\n",
    "    def __init__(self, model_name=\"google-t5/t5-11b\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "    def load(self, use_fp16=True, use_8bit=False):\n",
    "        \"\"\"Load T5 model with memory optimizations.\"\"\"\n",
    "        print(f\"Loading T5 encoder: {self.model_name}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(self.model_name)\n",
    "        \n",
    "        # Load model with optimizations\n",
    "        if use_8bit:\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            self.model = T5EncoderModel.from_pretrained(\n",
    "                self.model_name,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            print(\"✅ Loaded in 8-bit mode\")\n",
    "        else:\n",
    "            self.model = T5EncoderModel.from_pretrained(self.model_name)\n",
    "            if use_fp16:\n",
    "                self.model = self.model.half()\n",
    "                print(\"✅ Using FP16 precision\")\n",
    "            self.model = self.model.to(\"cuda\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        print(f\"✅ T5 encoder loaded\")\n",
    "        \n",
    "    def encode(self, text, max_length=77):\n",
    "        \"\"\"Encode text to embeddings.\"\"\"\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "        return {\n",
    "            \"encoder_hidden_states\": outputs.last_hidden_state,\n",
    "            \"attention_mask\": inputs.attention_mask\n",
    "        }\n",
    "    \n",
    "    def unload(self):\n",
    "        \"\"\"Free memory by unloading the model.\"\"\"\n",
    "        if self.model:\n",
    "            del self.model\n",
    "            self.model = None\n",
    "        if self.tokenizer:\n",
    "            del self.tokenizer\n",
    "            self.tokenizer = None\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"✅ T5 encoder unloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose T5 model based on available memory\n",
    "if gpu_memory >= 40:  # A100\n",
    "    t5_model = \"google-t5/t5-11b\"  # Best quality\n",
    "    print(f\"Using T5-11B (best quality) on {gpu_name}\")\n",
    "elif gpu_memory >= 16:  # T4 or similar\n",
    "    t5_model = \"google/flan-t5-xl\"  # Efficient\n",
    "    print(f\"Using Flan-T5-XL (efficient) on {gpu_name}\")\n",
    "else:\n",
    "    t5_model = \"google/flan-t5-base\"  # Minimal\n",
    "    print(f\"Using Flan-T5-Base (minimal) on {gpu_name}\")\n",
    "\n",
    "# Initialize and load T5 encoder\n",
    "t5_encoder = OptimizedT5Encoder(model_name=t5_model)\n",
    "t5_encoder.load(use_fp16=True, use_8bit=False)\n",
    "\n",
    "print(f\"💾 GPU memory allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encode Text Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts for robot manipulation tasks\n",
    "prompts = [\n",
    "    \"A robotic arm picks up white paper and places it into a red square target area on the table.\",\n",
    "    \"High-definition video of SO-100 robot manipulating paper with precise movements.\",\n",
    "    \"Robot gripper grasps paper and moves it to designated red square zone.\",\n",
    "    \"Automated paper handling: robot transfers white sheet to red target area.\",\n",
    "    \"The robot arm carefully picks up a sheet of paper from the table.\",\n",
    "]\n",
    "\n",
    "# Encode all prompts\n",
    "print(\"Encoding prompts...\")\n",
    "encoded_prompts = {}\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    encoded = t5_encoder.encode(prompt)\n",
    "    encoded_prompts[prompt] = encoded[\"encoder_hidden_states\"]\n",
    "    print(f\"  [{i}/{len(prompts)}] ✅ Encoded: '{prompt[:50]}...'\")\n",
    "\n",
    "print(f\"\\n💾 Current GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Cosmos Predict2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cosmos_predict2.inference import (\n",
    "    Video2WorldPipeline,\n",
    "    get_cosmos_predict2_video2world_pipeline,\n",
    ")\n",
    "\n",
    "print(f\"Loading Cosmos Predict2-{MODEL_SIZE} pipeline...\")\n",
    "\n",
    "# Create pipeline configuration\n",
    "config = get_cosmos_predict2_video2world_pipeline(model_size=MODEL_SIZE)\n",
    "\n",
    "# Update config to use our downloaded checkpoint\n",
    "config['dit_checkpoint_path'] = os.path.join(\n",
    "    checkpoint_dir,\n",
    "    \"model-720p-16fps.pt\"  # or \"model-720p-10fps.pt\" for 10fps\n",
    ")\n",
    "\n",
    "# Initialize pipeline\n",
    "try:\n",
    "    cosmos_pipe = Video2WorldPipeline.from_config(config)\n",
    "    cosmos_pipe = cosmos_pipe.to(\"cuda\")\n",
    "    cosmos_pipe.eval()\n",
    "    \n",
    "    print(f\"✅ Cosmos Predict2-{MODEL_SIZE} pipeline loaded successfully\")\n",
    "    print(f\"💾 Current GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading pipeline: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create or Load Input Video/Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from IPython.display import HTML, display, Image\n",
    "from PIL import Image as PILImage\n",
    "import base64\n",
    "\n",
    "def create_test_image(output_path=\"test_input.jpg\", width=1280, height=720):\n",
    "    \"\"\"Create a simple test image simulating a robot workspace.\"\"\"\n",
    "    # Create base image\n",
    "    img = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Add gradient background (table surface)\n",
    "    for y in range(height):\n",
    "        img[y, :] = [100 + int(50 * y / height), 80, 60]\n",
    "    \n",
    "    # Add white paper rectangle\n",
    "    paper_x, paper_y = width // 3, height // 2\n",
    "    paper_w, paper_h = 200, 150\n",
    "    cv2.rectangle(img, (paper_x, paper_y), (paper_x + paper_w, paper_y + paper_h), \n",
    "                  (255, 255, 255), -1)\n",
    "    cv2.rectangle(img, (paper_x, paper_y), (paper_x + paper_w, paper_y + paper_h), \n",
    "                  (200, 200, 200), 2)\n",
    "    \n",
    "    # Add red target square\n",
    "    target_x, target_y = 2 * width // 3, height // 2\n",
    "    target_size = 150\n",
    "    cv2.rectangle(img, (target_x, target_y), (target_x + target_size, target_y + target_size),\n",
    "                  (50, 50, 200), -1)\n",
    "    cv2.rectangle(img, (target_x, target_y), (target_x + target_size, target_y + target_size),\n",
    "                  (30, 30, 150), 3)\n",
    "    \n",
    "    # Add text labels\n",
    "    cv2.putText(img, \"Paper\", (paper_x + 70, paper_y - 10), \n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    cv2.putText(img, \"Target\", (target_x + 40, target_y - 10), \n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    \n",
    "    # Save image\n",
    "    cv2.imwrite(output_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "    print(f\"✅ Created test image: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "def display_image(image_path):\n",
    "    \"\"\"Display image in notebook.\"\"\"\n",
    "    img = PILImage.open(image_path)\n",
    "    display(img)\n",
    "\n",
    "# Create or upload input\n",
    "use_test_input = True  # Set to False to upload your own image/video\n",
    "\n",
    "if use_test_input:\n",
    "    input_path = create_test_image()\n",
    "    print(\"\\nTest input image:\")\n",
    "    display_image(input_path)\n",
    "else:\n",
    "    from google.colab import files\n",
    "    print(\"Please upload an image or video file:\")\n",
    "    uploaded = files.upload()\n",
    "    input_path = list(uploaded.keys())[0]\n",
    "    print(f\"✅ Uploaded: {input_path}\")\n",
    "    if input_path.endswith(('.jpg', '.jpeg', '.png')):\n",
    "        display_image(input_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Video with Cosmos Predict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decord\n",
    "from einops import rearrange\n",
    "import time\n",
    "\n",
    "def generate_video_cosmos(input_path, prompt_embedding, num_frames=16, fps=16):\n",
    "    \"\"\"Generate video using Cosmos Predict2.\"\"\"\n",
    "    \n",
    "    # Load input frame\n",
    "    if input_path.endswith(('.jpg', '.jpeg', '.png')):\n",
    "        # Input is an image\n",
    "        img = PILImage.open(input_path)\n",
    "        frames = np.array(img)[np.newaxis, ...]  # Add time dimension\n",
    "    else:\n",
    "        # Input is a video - use first frame\n",
    "        vr = decord.VideoReader(input_path)\n",
    "        frames = vr[:1].asnumpy()\n",
    "    \n",
    "    # Prepare input tensor\n",
    "    frames_tensor = torch.from_numpy(frames).float() / 255.0\n",
    "    frames_tensor = rearrange(frames_tensor, \"t h w c -> 1 c t h w\")\n",
    "    frames_tensor = frames_tensor.to(\"cuda\")\n",
    "    \n",
    "    print(f\"📊 Input shape: {frames_tensor.shape}\")\n",
    "    print(f\"🎬 Generating {num_frames} frames at {fps} FPS...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = cosmos_pipe(\n",
    "                frames_tensor,\n",
    "                prompt_embedding,\n",
    "                num_frames=num_frames,\n",
    "                fps=fps,\n",
    "                seed=42\n",
    "            )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"✅ Generation complete in {generation_time:.2f} seconds\")\n",
    "    print(f\"⚡ Speed: {num_frames/generation_time:.2f} frames/second\")\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure generation parameters based on GPU\n",
    "if gpu_memory >= 40:  # A100\n",
    "    generation_params = {\n",
    "        \"num_frames\": 121,  # ~7.5 seconds at 16fps\n",
    "        \"fps\": 16\n",
    "    }\n",
    "    print(\"🚀 Using A100 optimized settings\")\n",
    "elif gpu_memory >= 16:  # T4\n",
    "    generation_params = {\n",
    "        \"num_frames\": 61,  # ~3.8 seconds at 16fps\n",
    "        \"fps\": 16\n",
    "    }\n",
    "    print(\"Using T4 optimized settings\")\n",
    "else:\n",
    "    generation_params = {\n",
    "        \"num_frames\": 16,  # 1 second at 16fps\n",
    "        \"fps\": 16\n",
    "    }\n",
    "    print(\"Using conservative settings\")\n",
    "\n",
    "print(f\"Generation parameters: {generation_params}\")\n",
    "\n",
    "# Select prompt and generate\n",
    "selected_prompt = prompts[0]  # Use first prompt\n",
    "print(f\"\\n📝 Selected prompt: '{selected_prompt[:80]}...'\")\n",
    "\n",
    "# Get the pre-encoded embedding\n",
    "prompt_embedding = encoded_prompts[selected_prompt]\n",
    "\n",
    "# Generate video\n",
    "print(\"\\n🎬 Starting video generation...\")\n",
    "output_video = generate_video_cosmos(\n",
    "    input_path,\n",
    "    prompt_embedding,\n",
    "    num_frames=generation_params['num_frames'],\n",
    "    fps=generation_params['fps']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save and Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import shutil\n",
    "\n",
    "def save_video(tensor, output_path=\"output_video.mp4\", fps=16, auto_backup=True):\n",
    "    \"\"\"Save tensor as video file with automatic Google Drive backup.\"\"\"\n",
    "    # Convert tensor to numpy\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        video = tensor.cpu().numpy()\n",
    "    else:\n",
    "        video = tensor\n",
    "    \n",
    "    # Rearrange dimensions if needed\n",
    "    if video.ndim == 5:  # B C T H W\n",
    "        video = video[0]  # Remove batch\n",
    "    if video.shape[0] == 3:  # C T H W\n",
    "        video = np.transpose(video, (1, 2, 3, 0))  # T H W C\n",
    "    \n",
    "    # Normalize to 0-255\n",
    "    if video.max() <= 1.0:\n",
    "        video = (video * 255).astype(np.uint8)\n",
    "    \n",
    "    # Save video locally\n",
    "    writer = imageio.get_writer(output_path, fps=fps)\n",
    "    for frame in video:\n",
    "        writer.append_data(frame)\n",
    "    writer.close()\n",
    "    \n",
    "    print(f\"✅ Saved video locally: {output_path}\")\n",
    "    \n",
    "    # Auto-backup to Google Drive\n",
    "    if auto_backup and drive_output_dir:\n",
    "        drive_path = os.path.join(drive_output_dir, os.path.basename(output_path))\n",
    "        shutil.copy2(output_path, drive_path)\n",
    "        print(f\"☁️ Backed up to Drive: {drive_path}\")\n",
    "        \n",
    "        # Save metadata\n",
    "        metadata_path = drive_path.replace('.mp4', '_metadata.txt')\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            f.write(f\"Prompt: {selected_prompt}\\n\")\n",
    "            f.write(f\"Frames: {generation_params['num_frames']}\\n\")\n",
    "            f.write(f\"FPS: {generation_params['fps']}\\n\")\n",
    "            f.write(f\"Model: Cosmos-Predict2-{MODEL_SIZE}\\n\")\n",
    "            f.write(f\"Timestamp: {datetime.now().isoformat()}\\n\")\n",
    "        print(f\"📝 Metadata saved\")\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "def display_video(video_path):\n",
    "    \"\"\"Display video in notebook.\"\"\"\n",
    "    video = open(video_path, 'rb').read()\n",
    "    encoded = base64.b64encode(video).decode('ascii')\n",
    "    display(HTML(f'''\n",
    "    <video width=\"640\" height=\"360\" controls autoplay loop>\n",
    "        <source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    '''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the generated video\n",
    "output_filename = f\"cosmos_output_{datetime.now().strftime('%H%M%S')}.mp4\"\n",
    "output_path = save_video(output_video, output_filename, fps=16, auto_backup=True)\n",
    "\n",
    "# Display the result\n",
    "print(\"\\n🎥 Generated video:\")\n",
    "display_video(output_path)\n",
    "\n",
    "# Optional download\n",
    "from google.colab import files\n",
    "download = input(\"\\nDownload video to your computer? (y/n): \")\n",
    "if download.lower() == 'y':\n",
    "    files.download(output_path)\n",
    "    print(\"✅ Download started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Batch Processing (Optional)\n",
    "\n",
    "Process multiple prompts efficiently with automatic Drive backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch process all prompts\n",
    "batch_process = True  # Set to True to process all prompts\n",
    "\n",
    "if batch_process:\n",
    "    results = {}\n",
    "    \n",
    "    print(f\"🎬 Batch processing {len(prompts)} prompts...\")\n",
    "    if drive_output_dir:\n",
    "        print(f\"📁 All outputs will be saved to: {drive_output_dir}\")\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"\\n[{i+1}/{len(prompts)}] Processing...\")\n",
    "        print(f\"  Prompt: {prompt[:60]}...\")\n",
    "        \n",
    "        try:\n",
    "            # Generate video\n",
    "            output = generate_video_cosmos(\n",
    "                input_path,\n",
    "                encoded_prompts[prompt],\n",
    "                num_frames=generation_params['num_frames'],\n",
    "                fps=generation_params['fps']\n",
    "            )\n",
    "            \n",
    "            # Save with descriptive filename\n",
    "            output_file = f\"batch_{i:02d}_{datetime.now().strftime('%H%M%S')}.mp4\"\n",
    "            save_video(output, output_file, fps=16, auto_backup=True)\n",
    "            results[prompt] = output_file\n",
    "            \n",
    "            # Clear cache between generations\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Failed: {e}\")\n",
    "            continue\n",
    "    \n",
    "    print(f\"\\n✅ Batch processing complete!\")\n",
    "    print(f\"Successfully generated {len(results)}/{len(prompts)} videos\")\n",
    "    \n",
    "    # Display summary\n",
    "    print(\"\\n📊 Results summary:\")\n",
    "    for prompt, file in results.items():\n",
    "        print(f\"  - {prompt[:40]}... -> {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Memory Management and Session Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display session status\n",
    "print(\"📊 Session Status:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "print(f\"Total GPU memory: {gpu_memory:.1f} GB\")\n",
    "print(f\"GPU allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "print(f\"GPU reserved: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
    "print(f\"GPU free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated())/1024**3:.2f} GB\")\n",
    "\n",
    "if drive_output_dir:\n",
    "    print(f\"\\n✅ Outputs saved to Google Drive:\")\n",
    "    print(f\"   {drive_output_dir}\")\n",
    "    print(\"\\n💾 Your outputs are safe even if the session disconnects!\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No Google Drive backup - outputs will be lost if session disconnects!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clean up memory\n",
    "cleanup = False  # Set to True to free all memory\n",
    "\n",
    "if cleanup:\n",
    "    print(\"🧹 Cleaning up memory...\")\n",
    "    \n",
    "    # Unload models\n",
    "    if 't5_encoder' in locals():\n",
    "        t5_encoder.unload()\n",
    "        del t5_encoder\n",
    "    \n",
    "    if 'cosmos_pipe' in locals():\n",
    "        del cosmos_pipe\n",
    "    \n",
    "    # Clear cache\n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"✅ Cleanup complete\")\n",
    "    print(f\"GPU allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "else:\n",
    "    print(\"💡 Set cleanup=True to free GPU memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and Troubleshooting\n",
    "\n",
    "### Memory Optimization:\n",
    "- **A100 (40GB)**: Can run T5-11B + Cosmos-14B with 121 frames\n",
    "- **T4 (16GB)**: Use Flan-T5-XL + Cosmos-5B with 61 frames\n",
    "- **Low memory**: Use 8-bit quantization or smaller models\n",
    "\n",
    "### Performance Tips:\n",
    "- Enable TF32 on A100 for 2-3x speedup\n",
    "- Use FP16 (half precision) for memory efficiency\n",
    "- Batch encode prompts before generation\n",
    "- Clear cache between generations in batch processing\n",
    "\n",
    "### Common Issues:\n",
    "1. **OOM Error**: Reduce `num_frames` or use smaller models\n",
    "2. **Slow generation**: Check GPU type, use appropriate settings\n",
    "3. **Import errors**: Restart runtime after installing packages\n",
    "4. **Drive not mounting**: Check browser permissions for Google Drive\n",
    "\n",
    "### Recovery from Disconnection:\n",
    "If your session disconnects but you had Drive mounted, your outputs are safe!\n",
    "Simply remount Drive and navigate to your output directory to access generated videos."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
