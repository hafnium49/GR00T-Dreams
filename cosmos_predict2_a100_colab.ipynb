{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosmos Predict2 Full Pipeline on A100\\n",
    "\\n",
    "This notebook runs both T5 encoding and Cosmos Predict2 inference on a single A100 GPU.\\n",
    "\\n",
    "**Requirements:**\\n",
    "- Google Colab with A100 runtime\\n",
    "- 40GB GPU memory\\n",
    "\\n",
    "**Note:** Make sure to select `Runtime > Change runtime type > A100 GPU` before running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment and Check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Optional: Mount Google Drive for automatic saving\n# RECOMMENDED: Set to True to prevent data loss\nmount_drive = True  # Set to True to auto-save outputs to Google Drive\n\nif mount_drive:\n    from google.colab import drive\n    drive.mount('/content/drive')\n    print(\"‚úÖ Google Drive mounted at /content/drive\")\n    \n    # Create output directory in Drive\n    import os\n    from datetime import datetime\n    \n    # Create timestamped folder for this session\n    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n    drive_output_dir = f\"/content/drive/MyDrive/cosmos_outputs_{timestamp}\"\n    os.makedirs(drive_output_dir, exist_ok=True)\n    print(f\"üìÅ Output directory created: {drive_output_dir}\")\n    print(\"‚ö†Ô∏è All outputs will be automatically saved to Google Drive\")\nelse:\n    print(\"‚ö†Ô∏è WARNING: Google Drive not mounted - outputs may be lost if runtime disconnects!\")\n    print(\"   Set mount_drive=True to enable automatic saving\")\n    drive_output_dir = None"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install required packages\n",
    "!pip install -q --upgrade pip\n",
    "\n",
    "# Install PyTorch with CUDA support\n",
    "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "# Install Cosmos Predict2\n",
    "!pip install -q \"cosmos-predict2[cu126]\"\n",
    "\n",
    "# Install other dependencies\n",
    "!pip install -q transformers accelerate bitsandbytes\n",
    "!pip install -q decord einops imageio[ffmpeg]\n",
    "!pip install -q opencv-python-headless pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Define prompts for paper manipulation task (same as original notebook)\nprompts = [\n    \"A robotic arm picks up white paper and places it into a red square target area on the table.\",\n    \"High-definition video of SO-101 robot manipulating paper with precise movements.\",\n    \"Robot gripper grasps paper and moves it to designated red square zone.\",\n    \"Automated paper handling: robot transfers white sheet to red target area.\",\n]\n\n# Encode all prompts\nprint(\"Encoding prompts...\")\nencoded_prompts = {}\n\nfor prompt in prompts:\n    encoded = t5_encoder.encode(prompt)\n    encoded_prompts[prompt] = encoded[\"encoder_hidden_states\"]\n    print(f\"‚úÖ Encoded: '{prompt[:40]}...' Shape: {encoded['encoder_hidden_states'].shape}\")\n\nprint(f\"\\nüíæ Current GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mount Google Drive (Optional)\\n",
    "Mount your Google Drive if you have videos or want to save outputs there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import numpy as np\nimport cv2\nfrom IPython.display import HTML, display\nimport base64\nfrom pathlib import Path\nfrom PIL import Image\n\ndef extract_first_frame_from_dataset():\n    \"\"\"Extract first frame from paper_return dataset if available.\"\"\"\n    dataset_path = Path(\"paper_return_filtered_dataset\")\n    \n    # Check if dataset exists (would need to be uploaded or mounted)\n    if dataset_path.exists():\n        video_files = list((dataset_path / \"videos\").glob(\"**/*.mp4\"))\n        \n        if video_files:\n            video_path = video_files[0]\n            print(f\"Using video from dataset: {video_path}\")\n            \n            # Extract first frame\n            cap = cv2.VideoCapture(str(video_path))\n            ret, frame = cap.read()\n            cap.release()\n            \n            if ret:\n                # Convert BGR to RGB\n                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                Image.fromarray(frame_rgb).save(\"input_frame.jpg\")\n                print(f\"‚úÖ Extracted frame from paper_return dataset\")\n                print(f\"   Shape: {frame.shape}\")\n                return \"input_frame.jpg\"\n    \n    return None\n\ndef create_test_video(output_path=\"test_input.mp4\", width=1280, height=720, fps=16, duration=1):\n    \"\"\"Create a simple test video with a moving object.\"\"\"\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n    \n    num_frames = int(fps * duration)\n    \n    for i in range(num_frames):\n        # Create a frame with gradient background\n        frame = np.zeros((height, width, 3), dtype=np.uint8)\n        \n        # Add gradient background\n        for y in range(height):\n            frame[y, :] = [int(255 * y / height), 100, 150]\n        \n        # Add moving circle (simulating object)\n        x = int(width * (0.2 + 0.6 * i / num_frames))\n        y = height // 2\n        cv2.circle(frame, (x, y), 50, (255, 255, 255), -1)\n        \n        # Add text\n        cv2.putText(frame, \"Test Input\", (50, 50), \n                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n        \n        out.write(frame)\n    \n    out.release()\n    print(f\"Created test video: {output_path}\")\n    return output_path\n\ndef display_video(video_path):\n    \"\"\"Display video in notebook.\"\"\"\n    video = open(video_path, 'rb').read()\n    encoded = base64.b64encode(video).decode('ascii')\n    display(HTML(f'''\n    <video width=\"640\" height=\"360\" controls>\n        <source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\">\n    </video>\n    '''))\n\n# Try to extract from paper_return dataset first\ninput_image_path = extract_first_frame_from_dataset()\n\nif input_image_path is None:\n    # Create or upload video if dataset not available\n    use_test_video = True  # Set to False if you want to upload your own\n    \n    if use_test_video:\n        input_video_path = create_test_video()\n        # Extract first frame from test video\n        cap = cv2.VideoCapture(input_video_path)\n        ret, frame = cap.read()\n        cap.release()\n        if ret:\n            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            input_image_path = \"input_frame.jpg\"\n            Image.fromarray(frame_rgb).save(input_image_path)\n            print(f\"‚úÖ Saved input frame to: {input_image_path}\")\n    else:\n        from google.colab import files\n        print(\"Please upload a video file:\")\n        uploaded = files.upload()\n        input_video_path = list(uploaded.keys())[0]\n        print(f\"Uploaded: {input_video_path}\")\n        # Extract first frame\n        cap = cv2.VideoCapture(input_video_path)\n        ret, frame = cap.read()\n        cap.release()\n        if ret:\n            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            input_image_path = \"input_frame.jpg\"\n            Image.fromarray(frame_rgb).save(input_image_path)\n\n# Display the input frame\nif input_image_path and os.path.exists(input_image_path):\n    img = Image.open(input_image_path)\n    display(img)\n    print(f\"Input image: {input_image_path}, Size: {img.size}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "import decord\nfrom einops import rearrange\nimport time\n\ndef generate_video_cosmos(input_path, prompt_embedding, num_frames=16, fps=16):\n    \"\"\"Generate video using Cosmos Predict2 with same parameters as original notebook.\"\"\"\n    \n    # Load input frame\n    if input_path.endswith(('.jpg', '.jpeg', '.png')):\n        # Input is an image\n        from PIL import Image\n        img = Image.open(input_path)\n        frames = np.array(img)[np.newaxis, ...]  # Add time dimension\n    else:\n        # Input is a video\n        vr = decord.VideoReader(input_path)\n        frames = vr[:1].asnumpy()  # Get first frame\n    \n    # Prepare input tensor\n    frames_tensor = torch.from_numpy(frames).float() / 255.0\n    frames_tensor = rearrange(frames_tensor, \"t h w c -> 1 c t h w\")\n    frames_tensor = frames_tensor.to(\"cuda\")\n    \n    print(f\"Input shape: {frames_tensor.shape}\")\n    print(f\"Generating {num_frames} frames at {fps} FPS...\")\n    \n    start_time = time.time()\n    \n    with torch.no_grad():\n        with torch.cuda.amp.autocast():\n            output = cosmos_pipe(\n                frames_tensor,\n                prompt_embedding,\n                num_frames=num_frames,\n                fps=fps,\n                seed=42\n            )\n    \n    generation_time = time.time() - start_time\n    print(f\"‚úÖ Generation complete in {generation_time:.2f} seconds\")\n    print(f\"   Speed: {num_frames/generation_time:.2f} frames/second\")\n    \n    return output\n\n# Generation parameters matching original notebook\n# Start with lower resolution for testing, then scale up for A100\nbase_params = {\n    \"num_frames\": 8,  # Start with 8 frames, can increase to 16\n    \"fps\": 16\n}\n\n# Check GPU and adjust parameters\nif torch.cuda.is_available() and 'A100' in torch.cuda.get_device_name(0):\n    print(\"üöÄ A100 detected - Using optimized settings:\")\n    generation_params = {\n        \"num_frames\": 16,  # Match original notebook\n        \"fps\": 16\n    }\nelse:\n    print(\"Using conservative settings for non-A100 GPU:\")\n    generation_params = base_params\n\nprint(f\"Generation parameters: num_frames={generation_params['num_frames']}, fps={generation_params['fps']}\")\n\n# Select the first prompt (matching original notebook)\nselected_prompt = prompts[0]  # \"A robotic arm picks up white paper and places it into a red square target area on the table.\"\nprint(f\"\\nGenerating video for: '{selected_prompt[:50]}...'\")\n\n# Get the pre-encoded embedding\nprompt_embedding = encoded_prompts[selected_prompt]\n\n# Generate video\noutput_video = generate_video_cosmos(\n    input_image_path,\n    prompt_embedding,\n    num_frames=generation_params['num_frames'],\n    fps=generation_params['fps']\n)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import imageio\nimport shutil\n\ndef save_video(tensor, output_path=\"output_video.mp4\", fps=16, auto_backup=True):\n    \"\"\"Save tensor as video file with automatic Google Drive backup.\"\"\"\n    # Convert tensor to numpy\n    if isinstance(tensor, torch.Tensor):\n        video = tensor.cpu().numpy()\n    else:\n        video = tensor\n    \n    # Rearrange dimensions if needed\n    if video.ndim == 5:  # B C T H W\n        video = video[0]  # Remove batch\n    if video.shape[0] == 3:  # C T H W\n        video = np.transpose(video, (1, 2, 3, 0))  # T H W C\n    \n    # Normalize to 0-255\n    if video.max() <= 1.0:\n        video = (video * 255).astype(np.uint8)\n    \n    # Save video locally first\n    writer = imageio.get_writer(output_path, fps=fps)\n    for frame in video:\n        writer.append_data(frame)\n    writer.close()\n    \n    print(f\"‚úÖ Saved video locally: {output_path}\")\n    \n    # Auto-backup to Google Drive\n    if auto_backup and drive_output_dir:\n        drive_path = os.path.join(drive_output_dir, os.path.basename(output_path))\n        shutil.copy2(output_path, drive_path)\n        print(f\"‚òÅÔ∏è Backed up to Drive: {drive_path}\")\n        \n        # Also save metadata\n        metadata_path = drive_path.replace('.mp4', '_metadata.txt')\n        with open(metadata_path, 'w') as f:\n            f.write(f\"Prompt: {selected_prompt}\\n\")\n            f.write(f\"Frames: {generation_params['num_frames']}\\n\")\n            f.write(f\"FPS: {generation_params['fps']}\\n\")\n            f.write(f\"Timestamp: {datetime.now().isoformat()}\\n\")\n        print(f\"üìù Metadata saved: {metadata_path}\")\n    \n    return output_path\n\n# Save the generated video with auto-backup\noutput_filename = f\"cosmos_output_{datetime.now().strftime('%H%M%S')}.mp4\"\noutput_path = save_video(output_video, output_filename, fps=16, auto_backup=True)\n\n# Display the result\nprint(\"\\nGenerated video:\")\ndisplay_video(output_path)\n\n# Optional download (in addition to Drive backup)\ndownload_locally = input(\"\\nDownload to your computer too? (y/n): \")\nif download_locally.lower() == 'y':\n    from google.colab import files\n    files.download(output_path)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Process all prompts with auto-save to Drive\nbatch_process = True  # Set to True to process all prompts\n\nif batch_process:\n    results = {}\n    \n    print(f\"üé¨ Batch processing {len(prompts)} prompts...\")\n    if drive_output_dir:\n        print(f\"üìÅ All outputs will be saved to: {drive_output_dir}\")\n    \n    for i, prompt in enumerate(prompts):\n        print(f\"\\n[{i+1}/{len(prompts)}] Processing: {prompt[:50]}...\")\n        \n        try:\n            # Generate video\n            output = generate_video_cosmos(\n                input_image_path,\n                encoded_prompts[prompt],\n                num_frames=generation_params['num_frames'],\n                fps=generation_params['fps']\n            )\n            \n            # Save with descriptive filename\n            output_file = f\"output_{i:02d}_{datetime.now().strftime('%H%M%S')}.mp4\"\n            save_video(output, output_file, fps=16, auto_backup=True)\n            results[prompt] = output_file\n            \n            # Save batch progress to Drive\n            if drive_output_dir:\n                progress_file = os.path.join(drive_output_dir, \"batch_progress.txt\")\n                with open(progress_file, 'a') as f:\n                    f.write(f\"[{i+1}/{len(prompts)}] {prompt[:80]} -> {output_file}\\n\")\n            \n            # Clear cache between generations\n            torch.cuda.empty_cache()\n            \n        except Exception as e:\n            print(f\"  ‚ùå Failed: {e}\")\n            # Log errors to Drive\n            if drive_output_dir:\n                error_file = os.path.join(drive_output_dir, \"errors.txt\")\n                with open(error_file, 'a') as f:\n                    f.write(f\"Failed [{i+1}]: {prompt[:80]} - Error: {e}\\n\")\n            continue\n    \n    print(\"\\n‚úÖ Batch processing complete!\")\n    print(f\"Successfully generated {len(results)}/{len(prompts)} videos\")\n    \n    # Save summary to Drive\n    if drive_output_dir:\n        summary_file = os.path.join(drive_output_dir, \"summary.txt\")\n        with open(summary_file, 'w') as f:\n            f.write(f\"Batch Processing Summary\\n\")\n            f.write(f\"========================\\n\")\n            f.write(f\"Total prompts: {len(prompts)}\\n\")\n            f.write(f\"Successful: {len(results)}\\n\")\n            f.write(f\"Failed: {len(prompts) - len(results)}\\n\\n\")\n            for prompt, file in results.items():\n                f.write(f\"{prompt[:80]}...\\n  -> {file}\\n\\n\")\n        print(f\"üìä Summary saved to: {summary_file}\")\n    \n    for prompt, file in results.items():\n        print(f\"  - {prompt[:40]}... -> {file}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Memory usage and session management\nprint(\"Session Status:\")\nprint(f\"GPU allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\nprint(f\"GPU reserved: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\nprint(f\"GPU free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated())/1024**3:.2f} GB\")\n\nif drive_output_dir:\n    print(f\"\\n‚úÖ Outputs saved to Google Drive:\")\n    print(f\"   {drive_output_dir}\")\n    print(\"\\n‚ö†Ô∏è Your outputs are safe even if the session disconnects!\")\n    \n    # Create recovery script for next session\n    recovery_script = f\"\"\"# Recovery Script\n# Run this in a new session to continue from where you left off\n\nimport os\nfrom google.colab import drive\n\n# Mount Drive\ndrive.mount('/content/drive')\n\n# Previous output directory\noutput_dir = \"{drive_output_dir}\"\n\n# List generated videos\nimport glob\nvideos = glob.glob(os.path.join(output_dir, \"*.mp4\"))\nprint(f\"Found {{len(videos)}} generated videos:\")\nfor v in videos:\n    print(f\"  - {{os.path.basename(v)}}\")\n\n# Read progress\nif os.path.exists(os.path.join(output_dir, \"batch_progress.txt\")):\n    with open(os.path.join(output_dir, \"batch_progress.txt\"), 'r') as f:\n        print(\"\\\\nBatch Progress:\")\n        print(f.read())\n\"\"\"\n    \n    recovery_path = os.path.join(drive_output_dir, \"recovery_script.py\")\n    with open(recovery_path, 'w') as f:\n        f.write(recovery_script)\n    print(f\"üìÑ Recovery script saved: {recovery_path}\")\nelse:\n    print(\"\\n‚ö†Ô∏è No Google Drive backup - outputs will be lost if session disconnects!\")\n\n# Optional: Free memory\ncleanup = False  # Set to True to free all memory\n\nif cleanup:\n    print(\"\\nCleaning up...\")\n    \n    # Unload T5\n    if 't5_encoder' in locals():\n        t5_encoder.unload()\n    \n    # Unload Cosmos\n    if 'cosmos_pipe' in locals():\n        del cosmos_pipe\n    \n    # Clear cache\n    import gc\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    print(f\"‚úÖ Cleanup complete\")\n    print(f\"GPU allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose T5 model based on available memory\n",
    "gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "\n",
    "if gpu_memory >= 40:  # A100\n",
    "    # Can use T5-11B for best quality\n",
    "    t5_model = \"google-t5/t5-11b\"  # 22GB in FP16\n",
    "    print(f\"Using T5-11B (best quality) on A100\")\n",
    "elif gpu_memory >= 16:  # T4 or similar\n",
    "    # Use smaller model\n",
    "    t5_model = \"google/flan-t5-xl\"  # 3GB\n",
    "    print(f\"Using Flan-T5-XL (efficient) on limited GPU\")\n",
    "else:\n",
    "    t5_model = \"google/flan-t5-base\"  # <1GB\n",
    "    print(f\"Using Flan-T5-Base (minimal) on very limited GPU\")\n",
    "\n",
    "# Initialize and load T5 encoder\n",
    "t5_encoder = OptimizedT5Encoder(model_name=t5_model)\n",
    "t5_encoder.load(use_fp16=True, use_8bit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Encode Text Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts for paper manipulation task\n",
    "prompts = [\n",
    "    \"The robot picks up a piece of paper from the table\",\n",
    "    \"The robot folds the paper in half\",\n",
    "    \"The robot places the folded paper back on the table\",\n",
    "    \"The robot arm reaches for a sheet of paper\",\n",
    "    \"The robot grasps the paper with its gripper\",\n",
    "]\n",
    "\n",
    "# Encode all prompts\n",
    "print(\"Encoding prompts...\")\n",
    "encoded_prompts = {}\n",
    "\n",
    "for prompt in prompts:\n",
    "    encoded = t5_encoder.encode(prompt)\n",
    "    encoded_prompts[prompt] = encoded[\"encoder_hidden_states\"]\n",
    "    print(f\"‚úÖ Encoded: '{prompt[:40]}...' Shape: {encoded['encoder_hidden_states'].shape}\")\n",
    "\n",
    "print(f\"\\nüíæ Current GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Load Cosmos Predict2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cosmos_predict2.inference import (\n",
    "    Video2WorldPipeline,\n",
    "    get_cosmos_predict2_video2world_pipeline,\n",
    ")\n",
    "\n",
    "print(f\"Loading Cosmos Predict2-{MODEL_SIZE} pipeline...\")\n",
    "\n",
    "# Create pipeline configuration\n",
    "config = get_cosmos_predict2_video2world_pipeline(model_size=MODEL_SIZE)\n",
    "\n",
    "# Update config to use our downloaded checkpoint\n",
    "config['dit_checkpoint_path'] = os.path.join(\n",
    "    checkpoint_dir,\n",
    "    \"model-720p-16fps.pt\"  # or \"model-720p-10fps.pt\" for 10fps\n",
    ")\n",
    "\n",
    "# Initialize pipeline\n",
    "cosmos_pipe = Video2WorldPipeline.from_config(config)\n",
    "cosmos_pipe = cosmos_pipe.to(\"cuda\")\n",
    "cosmos_pipe.eval()\n",
    "\n",
    "print(f\"‚úÖ Cosmos pipeline loaded\")\n",
    "print(f\"üíæ Current GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Create or Load Input Video\\n",
    "You can either upload a video or create a simple test video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from IPython.display import HTML, display\n",
    "import base64\n",
    "\n",
    "def create_test_video(output_path=\"test_input.mp4\", width=1280, height=720, fps=16, duration=1):\n",
    "    \"\"\"Create a simple test video with a moving object.\"\"\"\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "    \n",
    "    num_frames = int(fps * duration)\n",
    "    \n",
    "    for i in range(num_frames):\n",
    "        # Create a frame with gradient background\n",
    "        frame = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "        \n",
    "        # Add gradient background\n",
    "        for y in range(height):\n",
    "            frame[y, :] = [int(255 * y / height), 100, 150]\n",
    "        \n",
    "        # Add moving circle (simulating object)\n",
    "        x = int(width * (0.2 + 0.6 * i / num_frames))\n",
    "        y = height // 2\n",
    "        cv2.circle(frame, (x, y), 50, (255, 255, 255), -1)\n",
    "        \n",
    "        # Add text\n",
    "        cv2.putText(frame, \"Test Input\", (50, 50), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "        \n",
    "        out.write(frame)\n",
    "    \n",
    "    out.release()\n",
    "    print(f\"Created test video: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "def display_video(video_path):\n",
    "    \"\"\"Display video in notebook.\"\"\"\n",
    "    video = open(video_path, 'rb').read()\n",
    "    encoded = base64.b64encode(video).decode('ascii')\n",
    "    display(HTML(f'''\n",
    "    <video width=\"640\" height=\"360\" controls>\n",
    "        <source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    '''))\n",
    "\n",
    "# Create or upload video\n",
    "use_test_video = True  # Set to False if you want to upload your own\n",
    "\n",
    "if use_test_video:\n",
    "    input_video_path = create_test_video()\n",
    "    display_video(input_video_path)\n",
    "else:\n",
    "    from google.colab import files\n",
    "    print(\"Please upload a video file:\")\n",
    "    uploaded = files.upload()\n",
    "    input_video_path = list(uploaded.keys())[0]\n",
    "    print(f\"Uploaded: {input_video_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Generate Video with Cosmos Predict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decord\n",
    "from einops import rearrange\n",
    "import time\n",
    "\n",
    "def generate_video_cosmos(input_path, prompt_embedding, num_frames=121, fps=16):\n",
    "    \"\"\"Generate video using Cosmos Predict2.\"\"\"\n",
    "    \n",
    "    # Load input video\n",
    "    vr = decord.VideoReader(input_path)\n",
    "    frames = vr[:1].asnumpy()  # Get first frame\n",
    "    \n",
    "    # Prepare input tensor\n",
    "    frames_tensor = torch.from_numpy(frames).float() / 255.0\n",
    "    frames_tensor = rearrange(frames_tensor, \"t h w c -> 1 c t h w\")\n",
    "    frames_tensor = frames_tensor.to(\"cuda\")\n",
    "    \n",
    "    print(f\"Input shape: {frames_tensor.shape}\")\n",
    "    print(f\"Generating {num_frames} frames at {fps} FPS...\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = cosmos_pipe(\n",
    "                frames_tensor,\n",
    "                prompt_embedding,\n",
    "                num_frames=num_frames,\n",
    "                fps=fps,\n",
    "                seed=42\n",
    "            )\n",
    "    \n",
    "    generation_time = time.time() - start_time\n",
    "    print(f\"‚úÖ Generation complete in {generation_time:.2f} seconds\")\n",
    "    print(f\"   Speed: {num_frames/generation_time:.2f} frames/second\")\n",
    "    \n",
    "    return output\n",
    "\n",
    "# Select a prompt and generate\n",
    "selected_prompt = prompts[0]  # \"The robot picks up a piece of paper from the table\"\n",
    "print(f\"\\nGenerating video for: '{selected_prompt}'\")\n",
    "\n",
    "# Get the pre-encoded embedding\n",
    "prompt_embedding = encoded_prompts[selected_prompt]\n",
    "\n",
    "# Generate video\n",
    "output_video = generate_video_cosmos(\n",
    "    input_video_path,\n",
    "    prompt_embedding,\n",
    "    num_frames=121,  # ~7.5 seconds at 16fps\n",
    "    fps=16\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Save and Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "\n",
    "def save_video(tensor, output_path=\"output_video.mp4\", fps=16):\n",
    "    \"\"\"Save tensor as video file.\"\"\"\n",
    "    # Convert tensor to numpy\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        video = tensor.cpu().numpy()\n",
    "    else:\n",
    "        video = tensor\n",
    "    \n",
    "    # Rearrange dimensions if needed\n",
    "    if video.ndim == 5:  # B C T H W\n",
    "        video = video[0]  # Remove batch\n",
    "    if video.shape[0] == 3:  # C T H W\n",
    "        video = np.transpose(video, (1, 2, 3, 0))  # T H W C\n",
    "    \n",
    "    # Normalize to 0-255\n",
    "    if video.max() <= 1.0:\n",
    "        video = (video * 255).astype(np.uint8)\n",
    "    \n",
    "    # Save video\n",
    "    writer = imageio.get_writer(output_path, fps=fps)\n",
    "    for frame in video:\n",
    "        writer.append_data(frame)\n",
    "    writer.close()\n",
    "    \n",
    "    print(f\"Saved video to: {output_path}\")\n",
    "    return output_path\n",
    "\n",
    "# Save the generated video\n",
    "output_path = save_video(output_video, \"cosmos_output.mp4\", fps=16)\n",
    "\n",
    "# Display the result\n",
    "print(\"\\nGenerated video:\")\n",
    "display_video(output_path)\n",
    "\n",
    "# Download option\n",
    "from google.colab import files\n",
    "download = input(\"Download the video? (y/n): \")\n",
    "if download.lower() == 'y':\n",
    "    files.download(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Batch Processing (Optional)\\n",
    "Process multiple prompts efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all prompts\n",
    "batch_process = False  # Set to True to process all prompts\n",
    "\n",
    "if batch_process:\n",
    "    results = {}\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"\\n[{i+1}/{len(prompts)}] Processing: {prompt[:50]}...\")\n",
    "        \n",
    "        # Generate video\n",
    "        output = generate_video_cosmos(\n",
    "            input_video_path,\n",
    "            encoded_prompts[prompt],\n",
    "            num_frames=61,  # Shorter for batch processing\n",
    "            fps=16\n",
    "        )\n",
    "        \n",
    "        # Save\n",
    "        output_file = f\"output_{i:02d}.mp4\"\n",
    "        save_video(output, output_file)\n",
    "        results[prompt] = output_file\n",
    "        \n",
    "        # Clear cache between generations\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"\\n‚úÖ Batch processing complete!\")\n",
    "    for prompt, file in results.items():\n",
    "        print(f\"  - {prompt[:40]}... -> {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Memory Management and Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Memory usage summary\n",
    "print(\"Memory Usage Summary:\")\n",
    "print(f\"GPU allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "print(f\"GPU reserved: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
    "print(f\"GPU free: {(torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated())/1024**3:.2f} GB\")\n",
    "\n",
    "# Optional: Free memory\n",
    "cleanup = False  # Set to True to free all memory\n",
    "\n",
    "if cleanup:\n",
    "    print(\"\\nCleaning up...\")\n",
    "    \n",
    "    # Unload T5\n",
    "    if 't5_encoder' in locals():\n",
    "        t5_encoder.unload()\n",
    "    \n",
    "    # Unload Cosmos\n",
    "    if 'cosmos_pipe' in locals():\n",
    "        del cosmos_pipe\n",
    "    \n",
    "    # Clear cache\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(f\"‚úÖ Cleanup complete\")\n",
    "    print(f\"GPU allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips and Troubleshooting\\n",
    "\\n",
    "### Memory Optimization:\\n",
    "- **A100 (40GB)**: Can run T5-11B + Cosmos-14B\\n",
    "- **T4 (16GB)**: Use Flan-T5-XL + Cosmos-2B\\n",
    "- **Low memory**: Use 8-bit quantization or unload T5 after encoding\\n",
    "\\n",
    "### Performance Tips:\\n",
    "- Enable TF32 on A100 for 2-3x speedup\\n",
    "- Use FP16 (half precision) for memory efficiency\\n",
    "- Batch encode prompts before generation\\n",
    "\\n",
    "### Common Issues:\\n",
    "1. **OOM Error**: Reduce batch size or use smaller models\\n",
    "2. **Slow generation**: Check GPU type, use smaller num_frames\\n",
    "3. **Import errors**: Restart runtime after installing packages"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "provenance": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}