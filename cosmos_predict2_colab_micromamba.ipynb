{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosmos Predict2 on Google Colab with Micromamba (Python 3.10)\n",
    "\n",
    "This notebook uses Micromamba to create a Python 3.10 environment for Cosmos-Predict2.\n",
    "\n",
    "**Why Micromamba?**\n",
    "- Fast and lightweight conda alternative\n",
    "- Can install Python 3.10 alongside any Colab runtime\n",
    "- Clean environment management\n",
    "- Works with Colab's Python 3.11/3.12\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU (A100, V100, or T4)\n",
    "- ~5 minutes for initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Check Current Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "print(\"ðŸ” Current Colab Environment:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python path: {sys.executable}\")\n",
    "\n",
    "# Check GPU\n",
    "try:\n",
    "    import torch\n",
    "    if torch.cuda.is_available():\n",
    "        gpu_name = torch.cuda.get_device_name(0)\n",
    "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"\\nGPU: {gpu_name}\")\n",
    "        print(f\"Memory: {gpu_memory:.1f} GB\")\n",
    "        print(f\"CUDA: {torch.version.cuda}\")\n",
    "    else:\n",
    "        print(\"\\nâš ï¸ No GPU detected - Please enable GPU in Runtime settings\")\n",
    "        gpu_memory = 0\n",
    "        gpu_name = \"None\"\n",
    "except:\n",
    "    print(\"\\nPyTorch not installed in Colab environment (this is fine)\")\n",
    "    gpu_memory = 16  # Default assumption\n",
    "    gpu_name = \"Unknown\"\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"\\nâœ… We'll install Python 3.10 with Micromamba for Cosmos-Predict2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install Micromamba"
   ]
  },
  {
   "cell_type": "code",
   "source": "from google.colab import drive\nfrom datetime import datetime\nimport os\n\n# Mount Google Drive for backup\nmount_drive = True  # Set to False only if you don't want backups\n\nif mount_drive:\n    try:\n        drive.mount('/content/drive')\n        \n        # Create session directory\n        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n        drive_output_dir = f\"/content/drive/MyDrive/cosmos_outputs_{timestamp}\"\n        os.makedirs(drive_output_dir, exist_ok=True)\n        \n        print(f\"âœ… Google Drive mounted successfully\")\n        print(f\"ðŸ“ Session directory: {drive_output_dir}\")\n        print(f\"ðŸ’¾ All outputs will be auto-saved here\")\n        \n        # Save session start info\n        with open(f\"{drive_output_dir}/session.txt\", 'w') as f:\n            f.write(f\"Session started: {datetime.now()}\\n\")\n            f.write(f\"Colab Python: {sys.version}\\n\")\n            if 'gpu_name' in locals():\n                f.write(f\"GPU: {gpu_name}\\n\")\n        \n        # Store for later use\n        os.environ['DRIVE_OUTPUT_DIR'] = drive_output_dir\n        \n    except Exception as e:\n        print(f\"âš ï¸ Could not mount Drive: {e}\")\n        print(\"Your work will only be saved locally (may be lost if session disconnects)\")\n        drive_output_dir = None\nelse:\n    print(\"âš ï¸ Skipping Drive mount - no automatic backups!\")\n    print(\"âš ï¸ Generated files will be LOST if session disconnects!\")\n    drive_output_dir = None",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 3: Install Micromamba",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"ðŸ“¦ Installing Micromamba...\"\n",
    "echo \"\"\n",
    "\n",
    "# Download and install micromamba\n",
    "curl -L https://micro.mamba.pm/api/micromamba/linux-64/latest | \\\n",
    "  tar -xvj bin/micromamba -O > /usr/local/bin/micromamba 2>/dev/null\n",
    "\n",
    "chmod +x /usr/local/bin/micromamba\n",
    "\n",
    "# Initialize micromamba\n",
    "export MAMBA_ROOT_PREFIX=/content/micromamba\n",
    "mkdir -p $MAMBA_ROOT_PREFIX\n",
    "\n",
    "# Verify installation\n",
    "/usr/local/bin/micromamba --version\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ… Micromamba installed successfully!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 4: Create Python 3.10 Environment"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"ðŸ Creating Python 3.10 environment...\"\n",
    "echo \"\"\n",
    "\n",
    "export MAMBA_ROOT_PREFIX=/content/micromamba\n",
    "\n",
    "# Create environment with Python 3.10\n",
    "/usr/local/bin/micromamba create -y -n cosmos310 python=3.10 pip -c conda-forge\n",
    "\n",
    "# Verify Python version\n",
    "echo \"\"\n",
    "echo \"Verifying Python version:\"\n",
    "/usr/local/bin/micromamba run -n cosmos310 python --version\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ… Python 3.10 environment created!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 5: Install Cosmos-Predict2 Dependencies"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"ðŸ“¦ Installing PyTorch and CUDA dependencies...\"\n",
    "echo \"This will take 2-3 minutes...\"\n",
    "echo \"\"\n",
    "\n",
    "export MAMBA_ROOT_PREFIX=/content/micromamba\n",
    "\n",
    "# Install PyTorch with CUDA support\n",
    "/usr/local/bin/micromamba run -n cosmos310 pip install \\\n",
    "  torch torchvision torchaudio \\\n",
    "  --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ… PyTorch installed\"\n",
    "\n",
    "# Verify CUDA\n",
    "echo \"\"\n",
    "echo \"Checking CUDA availability:\"\n",
    "/usr/local/bin/micromamba run -n cosmos310 python -c \\\n",
    "  \"import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'PyTorch version: {torch.__version__}')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"ðŸ“¦ Installing Cosmos-Predict2...\"\n",
    "echo \"This will take 3-4 minutes...\"\n",
    "echo \"\"\n",
    "\n",
    "export MAMBA_ROOT_PREFIX=/content/micromamba\n",
    "\n",
    "# Install Cosmos-Predict2\n",
    "/usr/local/bin/micromamba run -n cosmos310 pip install \\\n",
    "  \"cosmos-predict2[cu126]\" \\\n",
    "  --extra-index-url https://nvidia-cosmos.github.io/cosmos-dependencies/cu126_torch260/simple\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ… Cosmos-Predict2 installed\"\n",
    "\n",
    "# Install additional dependencies\n",
    "echo \"\"\n",
    "echo \"Installing additional dependencies...\"\n",
    "/usr/local/bin/micromamba run -n cosmos310 pip install \\\n",
    "  transformers accelerate bitsandbytes \\\n",
    "  decord einops \"imageio[ffmpeg]\" \\\n",
    "  opencv-python-headless pillow\n",
    "\n",
    "echo \"\"\n",
    "echo \"âœ… All dependencies installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 6: Verify Installation"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo \"ðŸ” Verifying installation...\"\n",
    "echo \"=\"*60\n",
    "echo \"\"\n",
    "\n",
    "export MAMBA_ROOT_PREFIX=/content/micromamba\n",
    "\n",
    "# Run verification script\n",
    "/usr/local/bin/micromamba run -n cosmos310 python -c \"\n",
    "import sys\n",
    "print(f'Python version: {sys.version}')\n",
    "print(f'Python path: {sys.executable}')\n",
    "print()\n",
    "\n",
    "# Check PyTorch\n",
    "try:\n",
    "    import torch\n",
    "    print(f'âœ… PyTorch {torch.__version__}')\n",
    "    print(f'   CUDA available: {torch.cuda.is_available()}')\n",
    "    if torch.cuda.is_available():\n",
    "        print(f'   GPU: {torch.cuda.get_device_name(0)}')\n",
    "except ImportError as e:\n",
    "    print(f'âŒ PyTorch import failed: {e}')\n",
    "\n",
    "# Check Cosmos-Predict2\n",
    "try:\n",
    "    from cosmos_predict2.inference import Video2WorldPipeline\n",
    "    print('âœ… Cosmos-Predict2 imported successfully!')\n",
    "except ImportError as e:\n",
    "    print(f'âŒ Cosmos-Predict2 import failed: {e}')\n",
    "\n",
    "# Check other dependencies\n",
    "try:\n",
    "    import transformers\n",
    "    print(f'âœ… Transformers {transformers.__version__}')\n",
    "except:\n",
    "    print('âŒ Transformers not found')\n",
    "\"\n",
    "\n",
    "echo \"\"\n",
    "echo \"=\"*60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 7: Create Helper Functions"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import tempfile\n",
    "import os\n",
    "import json\n",
    "\n",
    "def run_cosmos(code, return_output=False, verbose=True):\n",
    "    \"\"\"\n",
    "    Run Python code in the Cosmos Python 3.10 environment.\n",
    "    \n",
    "    Args:\n",
    "        code: Python code string to execute\n",
    "        return_output: If True, return output as string\n",
    "        verbose: If True, print output\n",
    "    \n",
    "    Returns:\n",
    "        Output string if return_output=True, else None\n",
    "    \"\"\"\n",
    "    # Write code to temporary file\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n",
    "        f.write(code)\n",
    "        temp_file = f.name\n",
    "    \n",
    "    try:\n",
    "        # Run with micromamba\n",
    "        cmd = [\n",
    "            '/usr/local/bin/micromamba', 'run', '-n', 'cosmos310',\n",
    "            'python', temp_file\n",
    "        ]\n",
    "        \n",
    "        env = os.environ.copy()\n",
    "        env['MAMBA_ROOT_PREFIX'] = '/content/micromamba'\n",
    "        \n",
    "        result = subprocess.run(\n",
    "            cmd,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            env=env\n",
    "        )\n",
    "        \n",
    "        if result.returncode != 0:\n",
    "            print(f\"âŒ Error running code:\")\n",
    "            print(result.stderr)\n",
    "            return None\n",
    "        \n",
    "        if verbose and result.stdout:\n",
    "            print(result.stdout)\n",
    "        \n",
    "        if return_output:\n",
    "            return result.stdout\n",
    "            \n",
    "    finally:\n",
    "        # Clean up temp file\n",
    "        if os.path.exists(temp_file):\n",
    "            os.unlink(temp_file)\n",
    "\n",
    "def run_cosmos_command(command):\n",
    "    \"\"\"\n",
    "    Run a shell command in the Cosmos environment.\n",
    "    \n",
    "    Args:\n",
    "        command: Shell command to execute\n",
    "    \"\"\"\n",
    "    full_cmd = f\"export MAMBA_ROOT_PREFIX=/content/micromamba && /usr/local/bin/micromamba run -n cosmos310 {command}\"\n",
    "    !{full_cmd}\n",
    "\n",
    "# Test the helper\n",
    "print(\"Testing helper function...\\n\")\n",
    "test_code = \"\"\"\n",
    "import sys\n",
    "print(f\"âœ… Running in Python {sys.version_info.major}.{sys.version_info.minor}\")\n",
    "print(\"Hello from Cosmos environment!\")\n",
    "\"\"\"\n",
    "run_cosmos(test_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 8: Download Model Checkpoints"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download models using the Cosmos environment\n",
    "download_code = f\"\"\"\n",
    "from huggingface_hub import snapshot_download\n",
    "import torch\n",
    "\n",
    "# Detect GPU and select model size\n",
    "if torch.cuda.is_available():\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    print(f\"GPU: {{gpu_name}} ({{gpu_memory:.1f}} GB)\")\n",
    "else:\n",
    "    gpu_memory = 16\n",
    "    print(\"No GPU detected, using defaults\")\n",
    "\n",
    "# Select model size\n",
    "if gpu_memory >= 40:\n",
    "    MODEL_SIZE = \"14B\"\n",
    "elif gpu_memory >= 24:\n",
    "    MODEL_SIZE = \"5B\"\n",
    "else:\n",
    "    MODEL_SIZE = \"2B\"\n",
    "\n",
    "print(f\"\\nSelected Cosmos-{{MODEL_SIZE}} model\")\n",
    "print(\"Downloading checkpoint (this may take 2-5 minutes)...\\n\")\n",
    "\n",
    "# Download\n",
    "checkpoint_dir = snapshot_download(\n",
    "    repo_id=f\"nvidia/Cosmos-Predict2-{{MODEL_SIZE}}-Video2World\",\n",
    "    cache_dir=\"/content/cosmos_checkpoints\",\n",
    "    resume_download=True\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ… Checkpoint downloaded to: {{checkpoint_dir}}\")\n",
    "\n",
    "# Save config for later use\n",
    "with open('/content/cosmos_config.txt', 'w') as f:\n",
    "    f.write(f\"{{MODEL_SIZE}},{{checkpoint_dir}}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸ“¥ Downloading Cosmos model checkpoint...\\n\")\n",
    "run_cosmos(download_code)\n",
    "\n",
    "# Read the config\n",
    "if os.path.exists('/content/cosmos_config.txt'):\n",
    "    with open('/content/cosmos_config.txt', 'r') as f:\n",
    "        MODEL_SIZE, checkpoint_dir = f.read().strip().split(',')\n",
    "    print(f\"\\nUsing Cosmos-{MODEL_SIZE} model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 9: Generate a Test Video"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete generation script\n",
    "generation_script = f\"\"\"\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "from cosmos_predict2.inference import (\n",
    "    Video2WorldPipeline,\n",
    "    get_cosmos_predict2_video2world_pipeline,\n",
    ")\n",
    "from einops import rearrange\n",
    "import imageio\n",
    "\n",
    "print(\"ðŸš€ Starting Cosmos-Predict2 generation...\\n\")\n",
    "\n",
    "# Read config\n",
    "with open('/content/cosmos_config.txt', 'r') as f:\n",
    "    MODEL_SIZE, checkpoint_dir = f.read().strip().split(',')\n",
    "\n",
    "print(f\"Using Cosmos-{{MODEL_SIZE}} model\")\n",
    "\n",
    "# Load T5 text encoder\n",
    "print(\"Loading T5 text encoder...\")\n",
    "t5_model = \"google/flan-t5-base\"  # Small model for testing\n",
    "tokenizer = T5Tokenizer.from_pretrained(t5_model)\n",
    "text_encoder = T5EncoderModel.from_pretrained(t5_model).half().to(\"cuda\")\n",
    "text_encoder.eval()\n",
    "print(\"âœ… T5 loaded\\n\")\n",
    "\n",
    "# Load Cosmos pipeline\n",
    "print(\"Loading Cosmos pipeline...\")\n",
    "config = get_cosmos_predict2_video2world_pipeline(model_size=MODEL_SIZE)\n",
    "\n",
    "# Find model file\n",
    "import glob\n",
    "model_files = glob.glob(f\"{{checkpoint_dir}}/**/*.pt\", recursive=True)\n",
    "if model_files:\n",
    "    config['dit_checkpoint_path'] = model_files[0]\n",
    "    print(f\"Using model: {{os.path.basename(model_files[0])}}\")\n",
    "\n",
    "cosmos_pipe = Video2WorldPipeline.from_config(config)\n",
    "cosmos_pipe = cosmos_pipe.to(\"cuda\")\n",
    "cosmos_pipe.eval()\n",
    "print(\"âœ… Cosmos pipeline loaded\\n\")\n",
    "\n",
    "# Create test input image\n",
    "print(\"Creating test input...\")\n",
    "img = np.ones((720, 1280, 3), dtype=np.uint8) * 100\n",
    "img[200:520, 400:880, :] = [200, 150, 100]  # Add colored rectangle\n",
    "img[300:400, 500:700, :] = [50, 100, 200]   # Add another shape\n",
    "test_image = Image.fromarray(img)\n",
    "test_image.save('/content/test_input.jpg')\n",
    "print(\"âœ… Test input created\\n\")\n",
    "\n",
    "# Encode text prompt\n",
    "prompt = \"A robotic arm moves smoothly across the table, picking up objects\"\n",
    "print(f\"Prompt: {{prompt}}\\n\")\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", max_length=77,\n",
    "                  padding=\"max_length\", truncation=True).to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    text_embeddings = text_encoder(**inputs).last_hidden_state\n",
    "\n",
    "# Prepare input image\n",
    "frames = np.array(test_image)[np.newaxis, ...]\n",
    "frames_tensor = torch.from_numpy(frames).float() / 255.0\n",
    "frames_tensor = rearrange(frames_tensor, \"t h w c -> 1 c t h w\").to(\"cuda\")\n",
    "\n",
    "# Generate video\n",
    "num_frames = 16  # Start with 16 frames for quick test\n",
    "print(f\"Generating {{num_frames}} frames...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output = cosmos_pipe(\n",
    "            frames_tensor,\n",
    "            text_embeddings,\n",
    "            num_frames=num_frames,\n",
    "            fps=8,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "print(\"âœ… Video generated!\\n\")\n",
    "\n",
    "# Convert and save video\n",
    "if isinstance(output, torch.Tensor):\n",
    "    video = output.cpu().numpy()\n",
    "else:\n",
    "    video = output\n",
    "\n",
    "# Fix dimensions\n",
    "if video.ndim == 5:\n",
    "    video = video[0]\n",
    "if video.shape[0] == 3:\n",
    "    video = np.transpose(video, (1, 2, 3, 0))\n",
    "if video.max() <= 1.0:\n",
    "    video = (video * 255).astype(np.uint8)\n",
    "\n",
    "# Save video\n",
    "output_path = '/content/cosmos_output.mp4'\n",
    "writer = imageio.get_writer(output_path, fps=8)\n",
    "for frame in video:\n",
    "    writer.append_data(frame)\n",
    "writer.close()\n",
    "\n",
    "print(f\"âœ… Video saved to: {{output_path}}\")\n",
    "print(f\"Generated {{len(video)}} frames at resolution {{video[0].shape[:2]}}\")\n",
    "print(f\"\\nMemory used: {{torch.cuda.memory_allocated()/1024**3:.2f}} GB\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"ðŸŽ¬ Generating test video with Cosmos-Predict2...\\n\")\n",
    "print(\"=\"*60)\n",
    "run_cosmos(generation_script)\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 10: Display Generated Video"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import HTML, Image, display\n",
    "import base64\n",
    "\n",
    "# Display input image\n",
    "if os.path.exists('/content/test_input.jpg'):\n",
    "    print(\"ðŸ“¸ Input image:\")\n",
    "    display(Image('/content/test_input.jpg', width=400))\n",
    "    print()\n",
    "\n",
    "# Display generated video\n",
    "if os.path.exists('/content/cosmos_output.mp4'):\n",
    "    print(\"ðŸŽ¥ Generated video:\")\n",
    "    \n",
    "    with open('/content/cosmos_output.mp4', 'rb') as f:\n",
    "        video_data = f.read()\n",
    "    \n",
    "    encoded = base64.b64encode(video_data).decode('ascii')\n",
    "    \n",
    "    display(HTML(f'''\n",
    "    <video width=\"640\" height=\"360\" controls autoplay loop>\n",
    "        <source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\">\n",
    "        Your browser does not support the video tag.\n",
    "    </video>\n",
    "    '''))\n",
    "    \n",
    "    print(\"\\nðŸŽ‰ Success! Cosmos-Predict2 is working correctly!\")\n",
    "    \n",
    "    # Download button\n",
    "    from google.colab import files\n",
    "    download = input(\"\\nDownload the video? (y/n): \")\n",
    "    if download.lower() == 'y':\n",
    "        files.download('/content/cosmos_output.mp4')\n",
    "else:\n",
    "    print(\"âŒ Video file not found. Check the output from the previous cell.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Step 11: Save Results to Google Drive\n\nSave your generated videos to Google Drive to prevent data loss. This is separate from mounting to avoid interrupting your workflow."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport shutil\nfrom datetime import datetime\n\n# Check if Drive was mounted earlier\ndrive_output_dir = os.environ.get('DRIVE_OUTPUT_DIR', None)\n\nif drive_output_dir and os.path.exists(drive_output_dir):\n    print(f\"ðŸ“ Saving to Drive: {drive_output_dir}\\n\")\n    \n    saved_files = []\n    \n    # Save generated video\n    if os.path.exists('/content/cosmos_output.mp4'):\n        output_name = f\"cosmos_output_{datetime.now().strftime('%H%M%S')}.mp4\"\n        drive_path = os.path.join(drive_output_dir, output_name)\n        shutil.copy2('/content/cosmos_output.mp4', drive_path)\n        saved_files.append(output_name)\n        print(f\"âœ… Saved video: {output_name}\")\n    \n    # Save input image\n    if os.path.exists('/content/test_input.jpg'):\n        input_name = f\"test_input_{datetime.now().strftime('%H%M%S')}.jpg\"\n        drive_path = os.path.join(drive_output_dir, input_name)\n        shutil.copy2('/content/test_input.jpg', drive_path)\n        saved_files.append(input_name)\n        print(f\"âœ… Saved input: {input_name}\")\n    \n    # Save generation metadata\n    metadata_file = os.path.join(drive_output_dir, f\"generation_{datetime.now().strftime('%H%M%S')}.txt\")\n    with open(metadata_file, 'w') as f:\n        f.write(f\"Generation completed: {datetime.now()}\\n\")\n        f.write(f\"Model: Cosmos-{MODEL_SIZE if 'MODEL_SIZE' in locals() else 'Unknown'}\\n\")\n        f.write(f\"Prompt: A robotic arm moves smoothly across the table, picking up objects\\n\")\n        f.write(f\"Frames: 16\\n\")\n        f.write(f\"FPS: 8\\n\")\n        f.write(f\"\\nFiles saved:\\n\")\n        for file in saved_files:\n            f.write(f\"  - {file}\\n\")\n    \n    print(f\"âœ… Saved metadata: {os.path.basename(metadata_file)}\")\n    \n    print(f\"\\nðŸ’¾ All files backed up to Google Drive!\")\n    print(f\"ðŸ“ Location: {drive_output_dir}\")\n    print(\"\\nâœ¨ Your work is safe even if the session disconnects!\")\n    \nelif drive_output_dir:\n    print(f\"âš ï¸ Drive directory not found: {drive_output_dir}\")\n    print(\"Files may not have been saved to Drive\")\nelse:\n    print(\"âš ï¸ Google Drive was not mounted earlier\")\n    print(\"Files are only available in this session and will be lost if it disconnects!\")\n    print(\"\\nTo save files now:\")\n    print(\"1. Run Step 2 (Mount Google Drive)\")\n    print(\"2. Run this cell again\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Complete! \n",
    "\n",
    "You now have Cosmos-Predict2 running on Google Colab with Python 3.10!\n",
    "\n",
    "### How to Use:\n",
    "\n",
    "1. **Run any Cosmos code** using the helper function:\n",
    "```python\n",
    "code = \"\"\"\n",
    "# Your Cosmos-Predict2 code here\n",
    "from cosmos_predict2.inference import Video2WorldPipeline\n",
    "# ...\n",
    "\"\"\"\n",
    "run_cosmos(code)\n",
    "```\n",
    "\n",
    "2. **Run commands** in the environment:\n",
    "```python\n",
    "run_cosmos_command(\"pip list\")  # List installed packages\n",
    "run_cosmos_command(\"python script.py\")  # Run a script\n",
    "```\n",
    "\n",
    "### Tips:\n",
    "- Save outputs to `/content/` to access from the notebook\n",
    "- Mount Google Drive to preserve outputs\n",
    "- Increase `num_frames` for longer videos (16, 61, 121)\n",
    "- Use different prompts for various motions\n",
    "\n",
    "### GPU Memory Guide:\n",
    "| GPU | Memory | Model | Max Frames |\n",
    "|-----|--------|-------|------------|\n",
    "| A100 | 40GB | 14B | 121 frames |\n",
    "| V100 | 16GB | 2B | 30-60 frames |\n",
    "| T4 | 16GB | 2B | 16-30 frames |\n",
    "\n",
    "### Troubleshooting:\n",
    "- **Import errors**: Check Step 5 verification output\n",
    "- **OOM errors**: Reduce `num_frames` or use smaller model\n",
    "- **No GPU**: Enable GPU in Runtime settings\n",
    "- **Slow download**: Models are large (2-10GB), be patient\n",
    "\n",
    "### What's Next:\n",
    "1. Try different prompts for various robot motions\n",
    "2. Upload your own input images\n",
    "3. Generate longer videos with more frames\n",
    "4. Fine-tune on your own data"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Custom generation with auto-save\ndef generate_and_save(prompt, num_frames=16, fps=8, save_to_drive=True):\n    \"\"\"\n    Generate a video with custom prompt and automatically save to Drive.\n    \"\"\"\n    generation_code = f'''\nimport os\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom transformers import T5EncoderModel, T5Tokenizer\nfrom cosmos_predict2.inference import (\n    Video2WorldPipeline,\n    get_cosmos_predict2_video2world_pipeline,\n)\nfrom einops import rearrange\nimport imageio\nfrom datetime import datetime\n\n# Read config\nwith open('/content/cosmos_config.txt', 'r') as f:\n    MODEL_SIZE, checkpoint_dir = f.read().strip().split(',')\n\n# Load models (reuse if already loaded)\nprint(\"Loading models...\")\nt5_model = \"google/flan-t5-base\"\ntokenizer = T5Tokenizer.from_pretrained(t5_model)\ntext_encoder = T5EncoderModel.from_pretrained(t5_model).half().to(\"cuda\")\ntext_encoder.eval()\n\nconfig = get_cosmos_predict2_video2world_pipeline(model_size=MODEL_SIZE)\nimport glob\nmodel_files = glob.glob(f\"{{checkpoint_dir}}/**/*.pt\", recursive=True)\nif model_files:\n    config['dit_checkpoint_path'] = model_files[0]\n\ncosmos_pipe = Video2WorldPipeline.from_config(config)\ncosmos_pipe = cosmos_pipe.to(\"cuda\")\ncosmos_pipe.eval()\nprint(\"âœ… Models loaded\")\n\n# Generate with custom prompt\nprompt = \"\"\"{prompt}\"\"\"\nprint(f\"\\\\nPrompt: {{prompt}}\")\nprint(f\"Settings: {{num_frames}} frames at {{fps}} fps\\\\n\")\n\n# Use existing input or create new one\nif os.path.exists('/content/test_input.jpg'):\n    test_image = Image.open('/content/test_input.jpg')\nelse:\n    img = np.ones((720, 1280, 3), dtype=np.uint8) * 100\n    img[200:520, 400:880, :] = [200, 150, 100]\n    test_image = Image.fromarray(img)\n    test_image.save('/content/test_input.jpg')\n\n# Encode prompt\ninputs = tokenizer(prompt, return_tensors=\"pt\", max_length=77,\n                  padding=\"max_length\", truncation=True).to(\"cuda\")\nwith torch.no_grad():\n    text_embeddings = text_encoder(**inputs).last_hidden_state\n\n# Prepare input\nframes = np.array(test_image)[np.newaxis, ...]\nframes_tensor = torch.from_numpy(frames).float() / 255.0\nframes_tensor = rearrange(frames_tensor, \"t h w c -> 1 c t h w\").to(\"cuda\")\n\n# Generate\nprint(\"Generating video...\")\nwith torch.no_grad():\n    with torch.cuda.amp.autocast():\n        output = cosmos_pipe(\n            frames_tensor,\n            text_embeddings,\n            num_frames={num_frames},\n            fps={fps},\n            seed=42\n        )\n\n# Convert and save\nif isinstance(output, torch.Tensor):\n    video = output.cpu().numpy()\nelse:\n    video = output\n\nif video.ndim == 5:\n    video = video[0]\nif video.shape[0] == 3:\n    video = np.transpose(video, (1, 2, 3, 0))\nif video.max() <= 1.0:\n    video = (video * 255).astype(np.uint8)\n\n# Save with timestamp\ntimestamp = datetime.now().strftime(\"%H%M%S\")\noutput_path = f'/content/cosmos_custom_{{timestamp}}.mp4'\nwriter = imageio.get_writer(output_path, fps={fps})\nfor frame in video:\n    writer.append_data(frame)\nwriter.close()\n\nprint(f\"\\\\nâœ… Video saved: {{output_path}}\")\nprint(f\"Generated {{len(video)}} frames\")\n\n# Save prompt info\nwith open(f'/content/prompt_{{timestamp}}.txt', 'w') as f:\n    f.write(prompt)\n'''\n    \n    # Run generation\n    print(f\"ðŸŽ¬ Generating video for: \\\"{prompt[:50]}...\\\"\")\n    run_cosmos(generation_code)\n    \n    # Auto-save to Drive if enabled\n    if save_to_drive and os.environ.get('DRIVE_OUTPUT_DIR'):\n        drive_dir = os.environ['DRIVE_OUTPUT_DIR']\n        timestamp = datetime.now().strftime(\"%H%M%S\")\n        \n        # Find the latest generated file\n        import glob\n        latest_video = max(glob.glob('/content/cosmos_custom_*.mp4'), \n                          key=os.path.getctime, default=None)\n        \n        if latest_video:\n            shutil.copy2(latest_video, f\"{drive_dir}/custom_{timestamp}.mp4\")\n            print(f\"â˜ï¸ Saved to Drive: custom_{timestamp}.mp4\")\n\n# Example usage\ncustom_prompts = [\n    \"A robotic gripper carefully picks up a delicate glass object\",\n    \"Industrial robot arm performs precise welding operations\",\n    \"Humanoid robot hand manipulates small electronic components\",\n]\n\nprint(\"Try generating with different prompts:\")\nprint(\"-\" * 60)\nfor i, p in enumerate(custom_prompts, 1):\n    print(f\"{i}. {p}\")\nprint(\"-\" * 60)\nprint(\"\\nExample usage:\")\nprint('generate_and_save(\"Your custom prompt here\", num_frames=16, fps=8)')",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 12: Generate More Videos (Optional)\n\nNow that everything is set up, you can generate more videos with different prompts or settings.",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}