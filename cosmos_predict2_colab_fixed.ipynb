{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosmos Predict2 Full Pipeline on Colab\n",
    "\n",
    "This notebook runs both T5 encoding and Cosmos Predict2 inference on Colab GPUs.\n",
    "\n",
    "**Important**: Cosmos-Predict2 requires Python 3.10, which is Colab's default. We'll use the system Python directly.\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with GPU runtime (A100, T4, or V100)\n",
    "- Python 3.10 (Colab default)\n",
    "\n",
    "**Note:** Select `Runtime > Change runtime type > GPU` (A100 recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Verify Python Version\n",
    "\n",
    "First, let's make sure we're using Python 3.10 (required by Cosmos-Predict2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Check Python version\n",
    "python_version = sys.version_info\n",
    "print(f\"ðŸ Python version: {python_version.major}.{python_version.minor}.{python_version.micro}\")\n",
    "\n",
    "if python_version.major == 3 and python_version.minor == 10:\n",
    "    print(\"âœ… Python 3.10 detected - compatible with Cosmos-Predict2\")\n",
    "else:\n",
    "    print(f\"âŒ Python {python_version.major}.{python_version.minor} detected\")\n",
    "    print(\"âš ï¸ Cosmos-Predict2 requires Python 3.10\")\n",
    "    print(\"Please use a Colab runtime with Python 3.10\")\n",
    "    raise RuntimeError(\"Incompatible Python version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Install uv Package Manager (Optional but Faster)\n",
    "\n",
    "We'll use `uv` for faster installation, but with the system Python directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose installation method\n",
    "USE_UV = True  # Set to False to use pip instead\n",
    "\n",
    "if USE_UV:\n",
    "    print(\"ðŸ“¦ Installing uv package manager for faster installation...\")\n",
    "    !curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "    \n",
    "    # Add uv to PATH\n",
    "    os.environ['PATH'] = f\"{os.path.expanduser('~/.local/bin')}:{os.environ['PATH']}\"\n",
    "    \n",
    "    # Verify uv installation\n",
    "    !uv --version\n",
    "    print(\"âœ… uv installed - will use for faster package installation\")\n",
    "else:\n",
    "    print(\"Using standard pip for installation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Install Cosmos-Predict2 and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture install_output\n",
    "\n",
    "if USE_UV:\n",
    "    print(\"Installing with uv (faster)...\")\n",
    "    # Use uv with system Python\n",
    "    !uv pip install --system --upgrade pip\n",
    "    !uv pip install --system \"cosmos-predict2[cu126]\" --extra-index-url https://nvidia-cosmos.github.io/cosmos-dependencies/cu126_torch260/simple\n",
    "    !uv pip install --system transformers accelerate bitsandbytes\n",
    "    !uv pip install --system decord einops \"imageio[ffmpeg]\"\n",
    "    !uv pip install --system opencv-python-headless pillow\n",
    "else:\n",
    "    print(\"Installing with pip...\")\n",
    "    !pip install --upgrade pip\n",
    "    !pip install \"cosmos-predict2[cu126]\" --extra-index-url https://nvidia-cosmos.github.io/cosmos-dependencies/cu126_torch260/simple\n",
    "    !pip install transformers accelerate bitsandbytes\n",
    "    !pip install decord einops \"imageio[ffmpeg]\"\n",
    "    !pip install opencv-python-headless pillow\n",
    "\n",
    "print(\"Installation complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show installation summary\n",
    "print(\"ðŸ“¦ Installation Summary:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if USE_UV:\n",
    "    print(\"âœ… Used uv for faster installation\")\n",
    "else:\n",
    "    print(\"âœ… Used pip for installation\")\n",
    "\n",
    "# Check if there were any errors\n",
    "install_text = install_output.stdout\n",
    "if \"error\" in install_text.lower() or \"failed\" in install_text.lower():\n",
    "    print(\"âš ï¸ Warning: Some errors during installation\")\n",
    "    print(\"You may need to restart runtime\")\n",
    "else:\n",
    "    print(\"âœ… All packages installed successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verify Installation and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datetime import datetime\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    \n",
    "    print(\"ðŸ–¥ï¸ GPU Information:\")\n",
    "    print(f\"  Device: {gpu_name}\")\n",
    "    print(f\"  Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"  CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"  PyTorch Version: {torch.__version__}\")\n",
    "    \n",
    "    # Classify GPU\n",
    "    if \"A100\" in gpu_name:\n",
    "        print(\"  ðŸš€ High-end GPU detected - can use largest models\")\n",
    "    elif \"T4\" in gpu_name or \"V100\" in gpu_name:\n",
    "        print(\"  âœ… Mid-range GPU detected - good performance expected\")\n",
    "    else:\n",
    "        print(\"  âš ï¸ Lower-end GPU - may need smaller models\")\n",
    "else:\n",
    "    print(\"âŒ No GPU detected! Cosmos-Predict2 requires GPU\")\n",
    "    gpu_memory = 0\n",
    "    gpu_name = \"CPU\"\n",
    "\n",
    "# Test Cosmos import\n",
    "try:\n",
    "    from cosmos_predict2.inference import Video2WorldPipeline\n",
    "    print(\"\\nâœ… Cosmos Predict2 imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"\\nâŒ Import error: {e}\")\n",
    "    print(\"Try restarting runtime: Runtime > Restart runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mount Google Drive (Recommended)\n",
    "\n",
    "Save outputs to Drive to prevent data loss if session disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "mount_drive = True  # Set to False to skip Drive mounting\n",
    "\n",
    "if mount_drive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Create output directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    drive_output_dir = f\"/content/drive/MyDrive/cosmos_outputs_{timestamp}\"\n",
    "    os.makedirs(drive_output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"âœ… Drive mounted\")\n",
    "    print(f\"ðŸ“ Output directory: {drive_output_dir}\")\n",
    "    print(\"ðŸ’¾ Outputs will be auto-saved to Drive\")\n",
    "    \n",
    "    # Save session info\n",
    "    with open(f\"{drive_output_dir}/session_info.txt\", \"w\") as f:\n",
    "        f.write(f\"Session started: {datetime.now()}\\n\")\n",
    "        f.write(f\"GPU: {gpu_name} ({gpu_memory:.1f} GB)\\n\")\n",
    "        f.write(f\"Python: {sys.version}\\n\")\n",
    "        f.write(f\"Installation method: {'uv' if USE_UV else 'pip'}\\n\")\n",
    "else:\n",
    "    print(\"âš ï¸ Drive not mounted - outputs may be lost if session disconnects\")\n",
    "    drive_output_dir = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Download Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Auto-select model size based on GPU\n",
    "if gpu_memory >= 40:  # A100\n",
    "    MODEL_SIZE = \"14B\"\n",
    "    print(\"ðŸš€ Selected Cosmos-14B (largest model)\")\n",
    "elif gpu_memory >= 24:  # A10G, 3090\n",
    "    MODEL_SIZE = \"5B\"\n",
    "    print(\"Selected Cosmos-5B (medium model)\")\n",
    "elif gpu_memory >= 15:  # T4, V100\n",
    "    MODEL_SIZE = \"2B\"\n",
    "    print(\"Selected Cosmos-2B (small model)\")\n",
    "else:\n",
    "    MODEL_SIZE = \"2B\"\n",
    "    print(\"âš ï¸ Limited GPU - using smallest model (2B)\")\n",
    "\n",
    "print(f\"\\nðŸ“¥ Downloading Cosmos-Predict2-{MODEL_SIZE} checkpoint...\")\n",
    "print(\"This will take 2-5 minutes...\")\n",
    "\n",
    "# Download checkpoint\n",
    "checkpoint_dir = snapshot_download(\n",
    "    repo_id=f\"nvidia/Cosmos-Predict2-{MODEL_SIZE}-Video2World\",\n",
    "    cache_dir=\"/content/cosmos_checkpoints\",\n",
    "    resume_download=True\n",
    ")\n",
    "\n",
    "print(f\"âœ… Checkpoint downloaded\")\n",
    "\n",
    "# Find model files\n",
    "import glob\n",
    "model_files = glob.glob(f\"{checkpoint_dir}/**/*.pt\", recursive=True)\n",
    "if model_files:\n",
    "    print(f\"Found {len(model_files)} model file(s)\")\n",
    "    for f in model_files[:3]:\n",
    "        print(f\"  - {os.path.basename(f)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Initialize T5 Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "\n",
    "# Auto-select T5 model\n",
    "if gpu_memory >= 40:\n",
    "    t5_model = \"google-t5/t5-11b\"\n",
    "    print(\"Using T5-11B (best quality)\")\n",
    "elif gpu_memory >= 24:\n",
    "    t5_model = \"google-t5/t5-3b\"\n",
    "    print(\"Using T5-3B (good quality)\")\n",
    "elif gpu_memory >= 15:\n",
    "    t5_model = \"google/flan-t5-xl\"\n",
    "    print(\"Using Flan-T5-XL (efficient)\")\n",
    "else:\n",
    "    t5_model = \"google/flan-t5-base\"\n",
    "    print(\"Using Flan-T5-Base (minimal)\")\n",
    "\n",
    "print(f\"Loading {t5_model}...\")\n",
    "\n",
    "# Load T5\n",
    "tokenizer = T5Tokenizer.from_pretrained(t5_model)\n",
    "model = T5EncoderModel.from_pretrained(t5_model).half().to(\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "print(f\"âœ… T5 encoder loaded\")\n",
    "print(f\"ðŸ’¾ Memory used: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "\n",
    "def encode_text(text):\n",
    "    \"\"\"Encode text to embeddings.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", max_length=77, \n",
    "                      padding=\"max_length\", truncation=True).to(\"cuda\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    return outputs.last_hidden_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Encode Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts\n",
    "prompts = [\n",
    "    \"A robotic arm picks up white paper and places it into a red square target area.\",\n",
    "    \"Robot gripper grasps paper and moves it to designated zone.\",\n",
    "    \"Automated paper handling: robot transfers sheet to target.\",\n",
    "]\n",
    "\n",
    "# Encode prompts\n",
    "print(\"ðŸ“ Encoding prompts...\")\n",
    "encoded_prompts = {}\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"  [{i}/{len(prompts)}] {prompt[:50]}...\")\n",
    "    encoded_prompts[prompt] = encode_text(prompt)\n",
    "\n",
    "print(f\"\\nâœ… Encoded {len(prompts)} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Load Cosmos Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cosmos_predict2.inference import (\n",
    "    Video2WorldPipeline,\n",
    "    get_cosmos_predict2_video2world_pipeline,\n",
    ")\n",
    "\n",
    "print(f\"ðŸš€ Loading Cosmos-{MODEL_SIZE} pipeline...\")\n",
    "\n",
    "# Get config\n",
    "config = get_cosmos_predict2_video2world_pipeline(model_size=MODEL_SIZE)\n",
    "\n",
    "# Find model file\n",
    "model_path = None\n",
    "for fps in ['16fps', '10fps']:\n",
    "    potential = os.path.join(checkpoint_dir, f\"model-720p-{fps}.pt\")\n",
    "    if os.path.exists(potential):\n",
    "        model_path = potential\n",
    "        break\n",
    "\n",
    "if not model_path and model_files:\n",
    "    model_path = model_files[0]\n",
    "\n",
    "if model_path:\n",
    "    config['dit_checkpoint_path'] = model_path\n",
    "    print(f\"Using: {os.path.basename(model_path)}\")\n",
    "\n",
    "# Load pipeline\n",
    "cosmos_pipe = Video2WorldPipeline.from_config(config)\n",
    "cosmos_pipe = cosmos_pipe.to(\"cuda\")\n",
    "cosmos_pipe.eval()\n",
    "\n",
    "print(f\"âœ… Pipeline loaded\")\n",
    "print(f\"ðŸ’¾ Total memory: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Create Input Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image as PILImage\n",
    "from IPython.display import display\n",
    "\n",
    "def create_test_image():\n",
    "    \"\"\"Create test image.\"\"\"\n",
    "    img = np.zeros((720, 1280, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Background\n",
    "    img[:, :] = [100, 80, 60]\n",
    "    \n",
    "    # White paper\n",
    "    cv2.rectangle(img, (400, 300), (600, 450), (255, 255, 255), -1)\n",
    "    \n",
    "    # Red target\n",
    "    cv2.rectangle(img, (800, 300), (950, 450), (50, 50, 200), -1)\n",
    "    \n",
    "    cv2.imwrite(\"input.jpg\", cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "    return \"input.jpg\"\n",
    "\n",
    "input_path = create_test_image()\n",
    "print(\"âœ… Created test input\")\n",
    "\n",
    "img = PILImage.open(input_path)\n",
    "display(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Generate Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from einops import rearrange\n",
    "import time\n",
    "\n",
    "# Configure generation\n",
    "if gpu_memory >= 40:\n",
    "    num_frames = 121\n",
    "elif gpu_memory >= 24:\n",
    "    num_frames = 61\n",
    "else:\n",
    "    num_frames = 16\n",
    "\n",
    "print(f\"Generating {num_frames} frames...\")\n",
    "\n",
    "# Load input\n",
    "img = PILImage.open(input_path)\n",
    "frames = np.array(img)[np.newaxis, ...]\n",
    "frames_tensor = torch.from_numpy(frames).float() / 255.0\n",
    "frames_tensor = rearrange(frames_tensor, \"t h w c -> 1 c t h w\").to(\"cuda\")\n",
    "\n",
    "# Generate\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    with torch.cuda.amp.autocast():\n",
    "        output = cosmos_pipe(\n",
    "            frames_tensor,\n",
    "            encoded_prompts[prompts[0]],\n",
    "            num_frames=num_frames,\n",
    "            fps=16,\n",
    "            seed=42\n",
    "        )\n",
    "\n",
    "print(f\"âœ… Generated in {time.time()-start:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save and Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Convert to video\n",
    "if isinstance(output, torch.Tensor):\n",
    "    video = output.cpu().numpy()\n",
    "else:\n",
    "    video = output\n",
    "\n",
    "if video.ndim == 5:\n",
    "    video = video[0]\n",
    "if video.shape[0] == 3:\n",
    "    video = np.transpose(video, (1, 2, 3, 0))\n",
    "if video.max() <= 1.0:\n",
    "    video = (video * 255).astype(np.uint8)\n",
    "\n",
    "# Save\n",
    "output_path = \"output.mp4\"\n",
    "writer = imageio.get_writer(output_path, fps=16)\n",
    "for frame in video:\n",
    "    writer.append_data(frame)\n",
    "writer.close()\n",
    "\n",
    "print(f\"âœ… Saved: {output_path}\")\n",
    "\n",
    "# Backup to Drive\n",
    "if drive_output_dir:\n",
    "    import shutil\n",
    "    drive_path = f\"{drive_output_dir}/output.mp4\"\n",
    "    shutil.copy2(output_path, drive_path)\n",
    "    print(f\"â˜ï¸ Backed up to Drive\")\n",
    "\n",
    "# Display\n",
    "with open(output_path, 'rb') as f:\n",
    "    video_data = f.read()\n",
    "encoded = base64.b64encode(video_data).decode('ascii')\n",
    "display(HTML(f'''\n",
    "<video width=\"640\" controls autoplay loop>\n",
    "    <source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\">\n",
    "</video>\n",
    "'''))\n",
    "\n",
    "# Download option\n",
    "from google.colab import files\n",
    "if input(\"Download? (y/n): \").lower() == 'y':\n",
    "    files.download(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **Python Version Error**: \n",
    "   - Cosmos-Predict2 requires Python 3.10\n",
    "   - Google Colab should have Python 3.10 by default\n",
    "   - If you see Python 3.12, try a different runtime\n",
    "\n",
    "2. **Import Error**:\n",
    "   - Restart runtime: `Runtime > Restart runtime`\n",
    "   - Then run cells again from the beginning\n",
    "\n",
    "3. **OOM (Out of Memory)**:\n",
    "   - Reduce `num_frames`\n",
    "   - Use smaller models\n",
    "   - Clear GPU cache: `torch.cuda.empty_cache()`\n",
    "\n",
    "4. **Installation Fails**:\n",
    "   - Try using pip instead of uv (set `USE_UV = False`)\n",
    "   - Check your Python version is 3.10\n",
    "\n",
    "### GPU Memory Guide:\n",
    "\n",
    "| GPU | Memory | T5 Model | Cosmos Model | Max Frames |\n",
    "|-----|--------|----------|--------------|------------|\n",
    "| A100 | 40GB | T5-11B | Cosmos-14B | 121 |\n",
    "| V100 | 16GB | Flan-T5-XL | Cosmos-2B | 16-30 |\n",
    "| T4 | 16GB | Flan-T5-XL | Cosmos-2B | 16 |\n",
    "\n",
    "### Tips:\n",
    "- Always mount Google Drive to save outputs\n",
    "- Use A100 for best results\n",
    "- Clear cache between generations\n",
    "- Restart runtime if imports fail"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}