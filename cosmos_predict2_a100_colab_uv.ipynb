{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosmos Predict2 Full Pipeline on A100 (with uv)\n",
    "\n",
    "This notebook runs both T5 encoding and Cosmos Predict2 inference on a single A100 GPU.\n",
    "Uses `uv` package manager for faster and more reliable installation (NVIDIA recommended).\n",
    "\n",
    "**Requirements:**\n",
    "- Google Colab with A100 runtime\n",
    "- 40GB GPU memory (or T4 with reduced settings)\n",
    "\n",
    "**Note:** Make sure to select `Runtime > Change runtime type > A100 GPU` before running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install uv Package Manager\n",
    "\n",
    "NVIDIA highly recommends using `uv` for Cosmos-Predict2 installation. It's much faster than pip and handles dependencies better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install uv package manager (NVIDIA recommended)\n",
    "!curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "\n",
    "# Add uv to PATH for this session\n",
    "import os\n",
    "import sys\n",
    "os.environ['PATH'] = f\"{os.path.expanduser('~/.local/bin')}:{os.environ['PATH']}\"\n",
    "sys.path.insert(0, os.path.expanduser('~/.local/bin'))\n",
    "\n",
    "# Verify uv installation\n",
    "!uv --version\n",
    "print(\"âœ… uv package manager installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Python Environment and Install Cosmos-Predict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Set installation method\nUSE_GITHUB = False  # Set to True for GitHub source, False for PyPI (recommended)\n\n# Create virtual environment with Python 3.10 (required by Cosmos)\n# Note: In Colab, uv always uses system Python, so no --system flag needed\n!uv venv --python 3.10 /content/cosmos_env\nprint(\"âœ… Created Python 3.10 virtual environment\")\n\n# Activate the environment for subsequent commands\nvenv_python = \"/content/cosmos_env/bin/python\"\nvenv_pip = \"/content/cosmos_env/bin/pip\"\nos.environ['VIRTUAL_ENV'] = '/content/cosmos_env'\nos.environ['PATH'] = f\"/content/cosmos_env/bin:{os.environ['PATH']}\"\n\n# Verify Python version\n!$venv_python --version\n\nprint(\"ðŸ“¦ Installing Cosmos-Predict2 with uv...\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "if USE_GITHUB:\n",
    "    # Clone and install from GitHub source\n",
    "    !git clone https://github.com/nvidia-cosmos/cosmos-predict2.git /content/cosmos-predict2\n",
    "    os.chdir('/content/cosmos-predict2')\n",
    "    \n",
    "    # Install with uv from source\n",
    "    !uv pip install -e \".[cu126]\" --extra-index-url https://nvidia-cosmos.github.io/cosmos-dependencies/cu126_torch260/simple\n",
    "    \n",
    "    COSMOS_PATH = '/content/cosmos-predict2'\n",
    "    sys.path.insert(0, COSMOS_PATH)\n",
    "    print(\"âœ… Installed Cosmos-Predict2 from GitHub source\")\n",
    "else:\n",
    "    # Install from PyPI with uv (recommended)\n",
    "    !uv pip install -U \"cosmos-predict2[cu126]\" --extra-index-url https://nvidia-cosmos.github.io/cosmos-dependencies/cu126_torch260/simple\n",
    "    \n",
    "    COSMOS_PATH = None\n",
    "    print(\"âœ… Installed Cosmos-Predict2 from PyPI\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Additional Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install additional dependencies with uv (much faster than pip)\n",
    "!uv pip install transformers accelerate bitsandbytes\n",
    "!uv pip install decord einops \"imageio[ffmpeg]\"\n",
    "!uv pip install opencv-python-headless pillow\n",
    "\n",
    "print(\"âœ… Additional dependencies installed with uv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Verify Installation and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Use the virtual environment's Python\n# Update sys.path to use the virtual environment\nimport sys\nif '/content/cosmos_env/lib/python3.10/site-packages' not in sys.path:\n    sys.path.insert(0, '/content/cosmos_env/lib/python3.10/site-packages')\n\nimport torch\nfrom datetime import datetime\n\n# Check GPU\nif torch.cuda.is_available():\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"ðŸ–¥ï¸ GPU: {gpu_name} ({gpu_memory:.1f} GB)\")\n    \n    # Check CUDA version\n    print(f\"ðŸ”§ CUDA Version: {torch.version.cuda}\")\n    print(f\"ðŸ”§ PyTorch Version: {torch.__version__}\")\nelse:\n    print(\"âŒ No GPU detected!\")\n    gpu_memory = 0\n    gpu_name = \"CPU\"\n\n# Verify we're using the right Python\nimport subprocess\nresult = subprocess.run(['which', 'python'], capture_output=True, text=True)\nprint(f\"ðŸ“ Using Python from: {result.stdout.strip()}\")\n\n# Test Cosmos import\ntry:\n    from cosmos_predict2.inference import Video2WorldPipeline\n    print(\"âœ… Cosmos Predict2 imports working correctly\")\nexcept ImportError as e:\n    print(f\"âŒ Import error: {e}\")\n    print(\"Trying to fix path...\")\n    if USE_GITHUB and COSMOS_PATH:\n        sys.path.insert(0, os.path.join(COSMOS_PATH, 'imaginaire'))"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Mount Google Drive (Recommended)\n",
    "\n",
    "Auto-save outputs to prevent data loss if session disconnects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive for automatic saving\n",
    "mount_drive = True  # Set to True to auto-save outputs\n",
    "\n",
    "if mount_drive:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    print(\"âœ… Google Drive mounted\")\n",
    "    \n",
    "    # Create timestamped output directory\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    drive_output_dir = f\"/content/drive/MyDrive/cosmos_outputs_{timestamp}\"\n",
    "    os.makedirs(drive_output_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"ðŸ“ Output directory: {drive_output_dir}\")\n",
    "    print(\"ðŸ’¾ All outputs will be automatically saved to Drive\")\n",
    "    \n",
    "    # Save session info\n",
    "    with open(f\"{drive_output_dir}/session_info.txt\", \"w\") as f:\n",
    "        f.write(f\"Session started: {datetime.now()}\\n\")\n",
    "        f.write(f\"GPU: {gpu_name} ({gpu_memory:.1f} GB)\\n\")\n",
    "        f.write(f\"Installation method: {'GitHub' if USE_GITHUB else 'PyPI'}\\n\")\n",
    "        f.write(f\"Using uv package manager\\n\")\n",
    "else:\n",
    "    print(\"âš ï¸ WARNING: Drive not mounted - outputs may be lost!\")\n",
    "    drive_output_dir = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Download Model Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "# Auto-select model size based on GPU\n",
    "if gpu_memory >= 40:  # A100\n",
    "    MODEL_SIZE = \"14B\"\n",
    "    print(\"ðŸš€ A100 detected - using largest model (14B)\")\n",
    "elif gpu_memory >= 24:  # A10G, 3090, 4090\n",
    "    MODEL_SIZE = \"5B\"\n",
    "    print(\"Using medium model (5B) for 24GB GPU\")\n",
    "elif gpu_memory >= 16:  # T4, V100\n",
    "    MODEL_SIZE = \"2B\"\n",
    "    print(\"Using small model (2B) for 16GB GPU\")\n",
    "else:\n",
    "    MODEL_SIZE = \"2B\"\n",
    "    print(\"âš ï¸ Limited GPU memory - using smallest model (2B)\")\n",
    "\n",
    "print(f\"\\nðŸ“¥ Downloading Cosmos-Predict2-{MODEL_SIZE} checkpoint...\")\n",
    "print(\"(This may take 2-5 minutes)\")\n",
    "\n",
    "# Download with progress\n",
    "checkpoint_dir = snapshot_download(\n",
    "    repo_id=f\"nvidia/Cosmos-Predict2-{MODEL_SIZE}-Video2World\",\n",
    "    cache_dir=\"/content/cosmos_checkpoints\",\n",
    "    resume_download=True\n",
    ")\n",
    "\n",
    "print(f\"âœ… Checkpoint downloaded: {checkpoint_dir}\")\n",
    "\n",
    "# Verify checkpoint files\n",
    "import glob\n",
    "model_files = glob.glob(f\"{checkpoint_dir}/*.pt\")\n",
    "if model_files:\n",
    "    print(f\"Found model files: {[os.path.basename(f) for f in model_files]}\")\n",
    "else:\n",
    "    print(\"âš ï¸ No .pt files found, checking subdirectories...\")\n",
    "    model_files = glob.glob(f\"{checkpoint_dir}/**/*.pt\", recursive=True)\n",
    "    if model_files:\n",
    "        print(f\"Found: {model_files[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Initialize T5 Text Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5EncoderModel, T5Tokenizer\n",
    "\n",
    "class OptimizedT5Encoder:\n",
    "    \"\"\"Memory-efficient T5 encoder with FP16 and 8-bit options.\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name=\"google-t5/t5-11b\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        \n",
    "    def load(self, use_fp16=True, use_8bit=False):\n",
    "        print(f\"ðŸ“š Loading T5 encoder: {self.model_name}\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(self.model_name)\n",
    "        \n",
    "        # Load model with optimizations\n",
    "        if use_8bit:\n",
    "            from transformers import BitsAndBytesConfig\n",
    "            quantization_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "            self.model = T5EncoderModel.from_pretrained(\n",
    "                self.model_name,\n",
    "                quantization_config=quantization_config,\n",
    "                device_map=\"auto\"\n",
    "            )\n",
    "            print(\"âœ… Loaded in 8-bit quantized mode\")\n",
    "        else:\n",
    "            self.model = T5EncoderModel.from_pretrained(self.model_name)\n",
    "            if use_fp16:\n",
    "                self.model = self.model.half()\n",
    "                print(\"âœ… Using FP16 precision\")\n",
    "            self.model = self.model.to(\"cuda\")\n",
    "        \n",
    "        self.model.eval()\n",
    "        print(f\"âœ… T5 encoder ready\")\n",
    "        \n",
    "    def encode(self, text, max_length=77):\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(**inputs)\n",
    "            \n",
    "        return {\n",
    "            \"encoder_hidden_states\": outputs.last_hidden_state,\n",
    "            \"attention_mask\": inputs.attention_mask\n",
    "        }\n",
    "    \n",
    "    def unload(self):\n",
    "        if self.model:\n",
    "            del self.model\n",
    "            self.model = None\n",
    "        if self.tokenizer:\n",
    "            del self.tokenizer\n",
    "            self.tokenizer = None\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"âœ… T5 encoder unloaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-select T5 model based on GPU memory\n",
    "if gpu_memory >= 40:  # A100\n",
    "    t5_model = \"google-t5/t5-11b\"  # Best quality, ~22GB in FP16\n",
    "    use_8bit = False\n",
    "    print(\"Using T5-11B (best quality)\")\n",
    "elif gpu_memory >= 24:\n",
    "    t5_model = \"google-t5/t5-3b\"  # Good balance\n",
    "    use_8bit = False\n",
    "    print(\"Using T5-3B (balanced)\")\n",
    "elif gpu_memory >= 16:\n",
    "    t5_model = \"google/flan-t5-xl\"  # Efficient, ~3GB\n",
    "    use_8bit = False\n",
    "    print(\"Using Flan-T5-XL (efficient)\")\n",
    "else:\n",
    "    t5_model = \"google/flan-t5-base\"  # Minimal, <1GB\n",
    "    use_8bit = False\n",
    "    print(\"Using Flan-T5-Base (minimal)\")\n",
    "\n",
    "# Initialize and load\n",
    "t5_encoder = OptimizedT5Encoder(model_name=t5_model)\n",
    "t5_encoder.load(use_fp16=True, use_8bit=use_8bit)\n",
    "\n",
    "print(f\"\\nðŸ’¾ GPU memory used: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "print(f\"ðŸ’¾ GPU memory free: {(gpu_memory - torch.cuda.memory_allocated()/1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Encode Text Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompts for robot manipulation\n",
    "prompts = [\n",
    "    \"A robotic arm picks up white paper and places it into a red square target area on the table.\",\n",
    "    \"High-definition video of SO-100 robot manipulating paper with precise movements.\",\n",
    "    \"Robot gripper grasps paper and moves it to designated red square zone.\",\n",
    "    \"Automated paper handling: robot transfers white sheet to red target area.\",\n",
    "    \"The robot arm carefully picks up a sheet of paper from the table.\",\n",
    "]\n",
    "\n",
    "print(\"ðŸ“ Encoding prompts with T5...\")\n",
    "encoded_prompts = {}\n",
    "\n",
    "for i, prompt in enumerate(prompts, 1):\n",
    "    print(f\"  [{i}/{len(prompts)}] {prompt[:50]}...\")\n",
    "    encoded = t5_encoder.encode(prompt)\n",
    "    encoded_prompts[prompt] = encoded[\"encoder_hidden_states\"]\n",
    "\n",
    "print(f\"\\nâœ… Encoded {len(prompts)} prompts\")\n",
    "print(f\"ðŸ’¾ Memory after encoding: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Load Cosmos Predict2 Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cosmos_predict2.inference import (\n",
    "    Video2WorldPipeline,\n",
    "    get_cosmos_predict2_video2world_pipeline,\n",
    ")\n",
    "\n",
    "print(f\"ðŸš€ Loading Cosmos-Predict2-{MODEL_SIZE} pipeline...\")\n",
    "\n",
    "# Get pipeline config\n",
    "config = get_cosmos_predict2_video2world_pipeline(model_size=MODEL_SIZE)\n",
    "\n",
    "# Find the correct model file\n",
    "model_path = None\n",
    "for fps in ['16fps', '10fps']:\n",
    "    potential_path = os.path.join(checkpoint_dir, f\"model-720p-{fps}.pt\")\n",
    "    if os.path.exists(potential_path):\n",
    "        model_path = potential_path\n",
    "        print(f\"Found model: {os.path.basename(model_path)}\")\n",
    "        break\n",
    "\n",
    "if not model_path:\n",
    "    # Search in subdirectories\n",
    "    import glob\n",
    "    pt_files = glob.glob(f\"{checkpoint_dir}/**/*.pt\", recursive=True)\n",
    "    if pt_files:\n",
    "        model_path = pt_files[0]\n",
    "        print(f\"Found model in subdirectory: {model_path}\")\n",
    "\n",
    "if model_path:\n",
    "    config['dit_checkpoint_path'] = model_path\n",
    "else:\n",
    "    print(\"âš ï¸ Model file not found, using default path\")\n",
    "\n",
    "# Initialize pipeline\n",
    "try:\n",
    "    cosmos_pipe = Video2WorldPipeline.from_config(config)\n",
    "    cosmos_pipe = cosmos_pipe.to(\"cuda\")\n",
    "    cosmos_pipe.eval()\n",
    "    \n",
    "    print(f\"âœ… Cosmos pipeline loaded successfully\")\n",
    "    print(f\"ðŸ’¾ Total GPU memory used: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error loading pipeline: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"1. Try restarting runtime\")\n",
    "    print(\"2. Check GPU memory\")\n",
    "    print(\"3. Try smaller model size\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Create Input Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from IPython.display import HTML, display, Image\n",
    "from PIL import Image as PILImage\n",
    "import base64\n",
    "\n",
    "def create_test_image(output_path=\"test_input.jpg\", width=1280, height=720):\n",
    "    \"\"\"Create a test image simulating a robot workspace.\"\"\"\n",
    "    img = np.zeros((height, width, 3), dtype=np.uint8)\n",
    "    \n",
    "    # Gradient background\n",
    "    for y in range(height):\n",
    "        img[y, :] = [100 + int(50 * y / height), 80, 60]\n",
    "    \n",
    "    # White paper\n",
    "    paper_x, paper_y = width // 3, height // 2\n",
    "    cv2.rectangle(img, (paper_x, paper_y), (paper_x + 200, paper_y + 150), \n",
    "                  (255, 255, 255), -1)\n",
    "    cv2.rectangle(img, (paper_x, paper_y), (paper_x + 200, paper_y + 150), \n",
    "                  (200, 200, 200), 2)\n",
    "    \n",
    "    # Red target\n",
    "    target_x = 2 * width // 3\n",
    "    cv2.rectangle(img, (target_x, paper_y), (target_x + 150, paper_y + 150),\n",
    "                  (50, 50, 200), -1)\n",
    "    cv2.rectangle(img, (target_x, paper_y), (target_x + 150, paper_y + 150),\n",
    "                  (30, 30, 150), 3)\n",
    "    \n",
    "    # Labels\n",
    "    cv2.putText(img, \"Paper\", (paper_x + 70, paper_y - 10), \n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    cv2.putText(img, \"Target\", (target_x + 40, paper_y - 10), \n",
    "               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    \n",
    "    cv2.imwrite(output_path, cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "    return output_path\n",
    "\n",
    "# Create or upload\n",
    "use_test = True  # Set False to upload your own\n",
    "\n",
    "if use_test:\n",
    "    input_path = create_test_image()\n",
    "    print(\"âœ… Created test input image\")\n",
    "    img = PILImage.open(input_path)\n",
    "    display(img)\n",
    "else:\n",
    "    from google.colab import files\n",
    "    print(\"Upload an image:\")\n",
    "    uploaded = files.upload()\n",
    "    input_path = list(uploaded.keys())[0]\n",
    "    print(f\"âœ… Using: {input_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Generate Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import decord\n",
    "from einops import rearrange\n",
    "import time\n",
    "\n",
    "def generate_video(input_path, prompt_embedding, num_frames=16, fps=16):\n",
    "    \"\"\"Generate video using Cosmos Predict2.\"\"\"\n",
    "    \n",
    "    # Load input\n",
    "    if input_path.endswith(('.jpg', '.jpeg', '.png')):\n",
    "        img = PILImage.open(input_path)\n",
    "        frames = np.array(img)[np.newaxis, ...]\n",
    "    else:\n",
    "        vr = decord.VideoReader(input_path)\n",
    "        frames = vr[:1].asnumpy()\n",
    "    \n",
    "    # Prepare tensor\n",
    "    frames_tensor = torch.from_numpy(frames).float() / 255.0\n",
    "    frames_tensor = rearrange(frames_tensor, \"t h w c -> 1 c t h w\").to(\"cuda\")\n",
    "    \n",
    "    print(f\"ðŸŽ¬ Generating {num_frames} frames at {fps} FPS...\")\n",
    "    start = time.time()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        with torch.cuda.amp.autocast():\n",
    "            output = cosmos_pipe(\n",
    "                frames_tensor,\n",
    "                prompt_embedding,\n",
    "                num_frames=num_frames,\n",
    "                fps=fps,\n",
    "                seed=42\n",
    "            )\n",
    "    \n",
    "    elapsed = time.time() - start\n",
    "    print(f\"âœ… Generated in {elapsed:.1f}s ({num_frames/elapsed:.1f} fps)\")\n",
    "    return output\n",
    "\n",
    "# Configure based on GPU\n",
    "if gpu_memory >= 40:\n",
    "    gen_params = {\"num_frames\": 121, \"fps\": 16}  # 7.5s\n",
    "elif gpu_memory >= 24:\n",
    "    gen_params = {\"num_frames\": 61, \"fps\": 16}  # 3.8s\n",
    "else:\n",
    "    gen_params = {\"num_frames\": 16, \"fps\": 16}  # 1s\n",
    "\n",
    "print(f\"Settings: {gen_params}\")\n",
    "\n",
    "# Generate\n",
    "selected_prompt = prompts[0]\n",
    "print(f\"\\nPrompt: {selected_prompt[:60]}...\")\n",
    "\n",
    "output_video = generate_video(\n",
    "    input_path,\n",
    "    encoded_prompts[selected_prompt],\n",
    "    **gen_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save and Display Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import shutil\n",
    "\n",
    "def save_video(tensor, path=\"output.mp4\", fps=16, backup=True):\n",
    "    \"\"\"Save video with optional Drive backup.\"\"\"\n",
    "    # Convert to numpy\n",
    "    if isinstance(tensor, torch.Tensor):\n",
    "        video = tensor.cpu().numpy()\n",
    "    else:\n",
    "        video = tensor\n",
    "    \n",
    "    # Fix dimensions\n",
    "    if video.ndim == 5:\n",
    "        video = video[0]\n",
    "    if video.shape[0] == 3:\n",
    "        video = np.transpose(video, (1, 2, 3, 0))\n",
    "    \n",
    "    # Normalize\n",
    "    if video.max() <= 1.0:\n",
    "        video = (video * 255).astype(np.uint8)\n",
    "    \n",
    "    # Save\n",
    "    writer = imageio.get_writer(path, fps=fps)\n",
    "    for frame in video:\n",
    "        writer.append_data(frame)\n",
    "    writer.close()\n",
    "    print(f\"âœ… Saved: {path}\")\n",
    "    \n",
    "    # Backup to Drive\n",
    "    if backup and drive_output_dir:\n",
    "        drive_path = os.path.join(drive_output_dir, os.path.basename(path))\n",
    "        shutil.copy2(path, drive_path)\n",
    "        print(f\"â˜ï¸ Backed up to Drive\")\n",
    "        \n",
    "        # Save metadata\n",
    "        meta_path = drive_path.replace('.mp4', '_info.txt')\n",
    "        with open(meta_path, 'w') as f:\n",
    "            f.write(f\"Prompt: {selected_prompt}\\n\")\n",
    "            f.write(f\"Model: Cosmos-{MODEL_SIZE}\\n\")\n",
    "            f.write(f\"Frames: {gen_params['num_frames']}\\n\")\n",
    "            f.write(f\"FPS: {gen_params['fps']}\\n\")\n",
    "            f.write(f\"Time: {datetime.now()}\\n\")\n",
    "    \n",
    "    return path\n",
    "\n",
    "def display_video(path):\n",
    "    \"\"\"Display video in notebook.\"\"\"\n",
    "    with open(path, 'rb') as f:\n",
    "        video = f.read()\n",
    "    encoded = base64.b64encode(video).decode('ascii')\n",
    "    display(HTML(f'''\n",
    "    <video width=\"640\" height=\"360\" controls autoplay loop>\n",
    "        <source src=\"data:video/mp4;base64,{encoded}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    '''))\n",
    "\n",
    "# Save and display\n",
    "output_file = f\"cosmos_{datetime.now().strftime('%H%M%S')}.mp4\"\n",
    "output_path = save_video(output_video, output_file, fps=16)\n",
    "\n",
    "print(\"\\nðŸŽ¥ Generated video:\")\n",
    "display_video(output_path)\n",
    "\n",
    "# Optional download\n",
    "from google.colab import files\n",
    "if input(\"\\nDownload? (y/n): \").lower() == 'y':\n",
    "    files.download(output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Batch Processing (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all prompts\n",
    "batch_process = True\n",
    "\n",
    "if batch_process:\n",
    "    results = {}\n",
    "    print(f\"ðŸŽ¬ Processing {len(prompts)} prompts...\\n\")\n",
    "    \n",
    "    for i, prompt in enumerate(prompts):\n",
    "        print(f\"[{i+1}/{len(prompts)}] {prompt[:50]}...\")\n",
    "        \n",
    "        try:\n",
    "            # Generate\n",
    "            output = generate_video(\n",
    "                input_path,\n",
    "                encoded_prompts[prompt],\n",
    "                **gen_params\n",
    "            )\n",
    "            \n",
    "            # Save\n",
    "            filename = f\"batch_{i:02d}_{datetime.now().strftime('%H%M%S')}.mp4\"\n",
    "            save_video(output, filename, fps=16)\n",
    "            results[prompt] = filename\n",
    "            \n",
    "            # Clear memory\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Failed: {e}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Complete: {len(results)}/{len(prompts)} succeeded\")\n",
    "    \n",
    "    # Save summary\n",
    "    if drive_output_dir:\n",
    "        summary_path = f\"{drive_output_dir}/batch_summary.txt\"\n",
    "        with open(summary_path, 'w') as f:\n",
    "            f.write(f\"Batch Processing Summary\\n\")\n",
    "            f.write(f\"========================\\n\")\n",
    "            f.write(f\"Total: {len(prompts)}\\n\")\n",
    "            f.write(f\"Success: {len(results)}\\n\\n\")\n",
    "            for p, f in results.items():\n",
    "                f.write(f\"{p}\\n  -> {f}\\n\\n\")\n",
    "        print(f\"ðŸ“ Summary saved to Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Memory Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Session status\n",
    "print(\"ðŸ“Š Session Status\")\n",
    "print(\"=\"*40)\n",
    "print(f\"GPU: {gpu_name}\")\n",
    "print(f\"Total Memory: {gpu_memory:.1f} GB\")\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "print(f\"Reserved: {torch.cuda.memory_reserved()/1024**3:.2f} GB\")\n",
    "print(f\"Free: {(gpu_memory - torch.cuda.memory_allocated()/1024**3):.2f} GB\")\n",
    "\n",
    "if drive_output_dir:\n",
    "    print(f\"\\nâœ… Outputs saved to Drive:\")\n",
    "    print(f\"   {drive_output_dir}\")\n",
    "    \n",
    "    # List saved files\n",
    "    import glob\n",
    "    saved_videos = glob.glob(f\"{drive_output_dir}/*.mp4\")\n",
    "    if saved_videos:\n",
    "        print(f\"\\nðŸ“¹ Saved {len(saved_videos)} videos:\")\n",
    "        for v in saved_videos[:5]:  # Show first 5\n",
    "            print(f\"   - {os.path.basename(v)}\")\n",
    "        if len(saved_videos) > 5:\n",
    "            print(f\"   ... and {len(saved_videos)-5} more\")\n",
    "\n",
    "# Cleanup option\n",
    "cleanup = False  # Set True to free memory\n",
    "\n",
    "if cleanup:\n",
    "    print(\"\\nðŸ§¹ Cleaning up...\")\n",
    "    \n",
    "    if 't5_encoder' in locals():\n",
    "        t5_encoder.unload()\n",
    "        del t5_encoder\n",
    "    \n",
    "    if 'cosmos_pipe' in locals():\n",
    "        del cosmos_pipe\n",
    "    \n",
    "    import gc\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"âœ… Memory freed\")\n",
    "    print(f\"Allocated: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips & Troubleshooting\n",
    "\n",
    "### Why use `uv`?\n",
    "- **3-10x faster** than pip for installing packages\n",
    "- **Better dependency resolution** - avoids conflicts\n",
    "- **NVIDIA recommended** for Cosmos-Predict2\n",
    "- **Consistent environments** - always uses Python 3.10 as required\n",
    "\n",
    "### GPU Memory Guide:\n",
    "| GPU | Memory | T5 Model | Cosmos Model | Max Frames |\n",
    "|-----|--------|----------|--------------|------------|\n",
    "| A100 | 40GB | T5-11B | Cosmos-14B | 121 |\n",
    "| A10G | 24GB | T5-3B | Cosmos-5B | 61 |\n",
    "| T4 | 16GB | Flan-T5-XL | Cosmos-2B | 16-30 |\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **Import Error**: Restart runtime after installation\n",
    "2. **OOM Error**: Reduce `num_frames` or use smaller models\n",
    "3. **Slow Generation**: Normal speeds are 1-5 fps for generation\n",
    "4. **uv not found**: Re-run the uv installation cell\n",
    "\n",
    "### Performance Tips:\n",
    "- Use FP16 precision (default)\n",
    "- Clear cache between batches\n",
    "- Mount Drive to prevent data loss\n",
    "- Use A100 for best performance\n",
    "\n",
    "### Recovery:\n",
    "If session disconnects but Drive was mounted, your videos are safe in:\n",
    "`/content/drive/MyDrive/cosmos_outputs_[timestamp]/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}