{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cosmos-Predict **2.5** on Google Colab with Micromamba (Python 3.10)\n",
        "\n",
        "This notebook sets up **Cosmos-Predict 2.5** following the [official setup guide](https://github.com/nvidia-cosmos/cosmos-predict2.5/blob/main/docs/setup.md).\n",
        "\n",
        "## Key Requirements\n",
        "\n",
        "**Hardware & Driver:**\n",
        "- **GPU**: NVIDIA Ampere or newer (RTX 30 Series, A100, H100, Blackwell)\n",
        "- **Driver**: >=570.124.06 (CUDA 12.8.1 compatible)\n",
        "- **VRAM**: ~32.5 GB for 2B@720p/16fps, ~24 GB for 2B@480p/10fps\n",
        "\n",
        "**Software:**\n",
        "- **Python**: 3.10 (required - project specifically targets this version)\n",
        "- **OS**: Linux x86-64 with glibc>=2.31\n",
        "- **PyTorch**: CUDA 12.8 compatible version\n",
        "\n",
        "## Why Micromamba?\n",
        "\n",
        "- **Python 3.10**: Colab defaults to 3.10+ but Micromamba ensures clean environment\n",
        "- **No uv**: `uv` doesn't work well with Colab's virtual environment system\n",
        "- **Fast & Clean**: Conda-compatible package manager for isolated environments\n",
        "- **CUDA Control**: Easy pinning of CUDA/PyTorch versions\n",
        "\n",
        "## Setup Time\n",
        "\n",
        "- **Initial setup**: ~5-10 minutes (environment + dependencies)\n",
        "- **First inference**: +5-10 minutes (checkpoint auto-download)\n",
        "- **Subsequent runs**: ~1-2 minutes\n",
        "\n",
        "## Colab Runtime\n",
        "\n",
        "- Select **GPU runtime**: Runtime ‚Üí Change runtime type ‚Üí GPU\n",
        "- **Recommended**: A100 (40GB or 80GB)\n",
        "- **Minimum**: GPU with Ampere architecture and sufficient VRAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Check Current Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys, os, platform\n",
        "print(\"üîç Current Colab Environment:\\n\" + \"=\"*60)\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(f\"Executable: {sys.executable}\")\n",
        "print(f\"Platform: {platform.platform()}\")\n",
        "\n",
        "# GPU probe\n",
        "try:\n",
        "    import torch\n",
        "    gpu_ok = torch.cuda.is_available()\n",
        "except Exception:\n",
        "    gpu_ok = False\n",
        "\n",
        "if gpu_ok:\n",
        "    name = torch.cuda.get_device_name(0)\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    vram_gb = props.total_memory / 1024**3\n",
        "    print(f\"\\nGPU: {name}\")\n",
        "    print(f\"VRAM: {vram_gb:.1f} GB\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No GPU detected ‚Äî enable GPU in Runtime ‚Üí Change runtime type ‚Üí T4/V100/A100‚Ä¶ (A100+ recommended)\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"\\nWe will create a **Python 3.10** env with Micromamba and install Torch (cu126) + Cosmos‚ÄëPredict 2.5.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: (Optional) Mount Google Drive for Autosave"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from datetime import datetime\n",
        "import os, sys\n",
        "\n",
        "MOUNT_DRIVE = True\n",
        "if MOUNT_DRIVE:\n",
        "    try:\n",
        "        drive.mount('/content/drive')\n",
        "        ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "        out_dir = f\"/content/drive/MyDrive/cosmos_outputs_{ts}\"\n",
        "        os.makedirs(out_dir, exist_ok=True)\n",
        "        os.environ['DRIVE_OUTPUT_DIR'] = out_dir\n",
        "        print(f\"‚úÖ Drive mounted | Session dir: {out_dir}\")\n",
        "        with open(f\"{out_dir}/session.txt\", 'w') as f:\n",
        "            f.write(f\"Session: {datetime.now()}\\n\")\n",
        "            f.write(f\"Colab Python: {sys.version}\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Could not mount Drive: {e}\\nFiles will be local only.\")\n",
        "else:\n",
        "    print(\"‚è≠Ô∏è Skipping Drive mount ‚Äî outputs will be ephemeral.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Install Micromamba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "echo \"üì¶ Installing Micromamba...\"\n",
        "curl -Ls https://micro.mamba.pm/api/micromamba/linux-64/latest | tar -xvj bin/micromamba -O > /usr/local/bin/micromamba 2>/dev/null\n",
        "chmod +x /usr/local/bin/micromamba\n",
        "export MAMBA_ROOT_PREFIX=/content/micromamba\n",
        "mkdir -p \"$MAMBA_ROOT_PREFIX\"\n",
        "/usr/local/bin/micromamba --version\n",
        "echo \"‚úÖ Micromamba installed\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Create a **Python 3.10** Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "export MAMBA_ROOT_PREFIX=/content/micromamba\n",
        "echo \"üêç Creating env: cosmos310 (Python 3.10)\"\n",
        "/usr/local/bin/micromamba create -y -n cosmos310 python=3.10 pip -c conda-forge\n",
        "/usr/local/bin/micromamba run -n cosmos310 python --version\n",
        "echo \"‚úÖ Env created\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Install **PyTorch (CUDA 12.8.1 compatible)** and **Cosmos‚ÄëPredict 2.5**\n",
        "\n",
        "Following the [official setup guide](https://github.com/nvidia-cosmos/cosmos-predict2.5/blob/main/docs/setup.md):\n",
        "- Requires NVIDIA driver >=570.124.06 (compatible with CUDA 12.8.1)\n",
        "- Python 3.10 (verified in Step 4)\n",
        "- Ampere+ architecture GPUs (A100, RTX 30 series or newer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "export MAMBA_ROOT_PREFIX=/content/micromamba\n",
        "\n",
        "echo \"\udd0d Checking NVIDIA driver version...\"\n",
        "nvidia-smi | grep \"CUDA Version:\" || echo \"‚ö†Ô∏è Driver check failed\"\n",
        "\n",
        "echo \"\"\n",
        "echo \"\ud83düì¶ Installing PyTorch with CUDA 12.8.1 support\"\n",
        "/usr/local/bin/micromamba run -n cosmos310 python - <<'PY'\n",
        "import sys, subprocess\n",
        "\n",
        "def pip(args):\n",
        "    cmd = [sys.executable, '-m', 'pip'] + args\n",
        "    print('>>>', ' '.join(cmd))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "# Upgrade pip first\n",
        "pip(['install', '--upgrade', 'pip'])\n",
        "\n",
        "# Install PyTorch with CUDA 12.8 support\n",
        "# Using cu128 wheels from PyTorch (CUDA 12.8.1 compatible)\n",
        "pip([\n",
        "    'install', '--upgrade',\n",
        "    'torch', 'torchvision', 'torchaudio',\n",
        "    '--index-url', 'https://download.pytorch.org/whl/cu128'\n",
        "])\n",
        "\n",
        "print(\"‚úÖ PyTorch installation complete\")\n",
        "PY\n",
        "\n",
        "echo \"\"\n",
        "echo \"üì• Cloning Cosmos-Predict 2.5 repository\"\n",
        "if [ ! -d \"/content/cosmos-predict2.5\" ]; then\n",
        "    git clone https://github.com/nvidia-cosmos/cosmos-predict2.5.git /content/cosmos-predict2.5\n",
        "    echo \"‚úÖ Repository cloned\"\n",
        "else\n",
        "    echo \"‚ÑπÔ∏è Repository already exists\"\n",
        "fi\n",
        "\n",
        "echo \"\"\n",
        "echo \"üì¶ Installing Cosmos-Predict 2.5 and dependencies\"\n",
        "cd /content/cosmos-predict2.5\n",
        "\n",
        "/usr/local/bin/micromamba run -n cosmos310 python - <<'PY'\n",
        "import sys, subprocess, os\n",
        "\n",
        "def pip(args):\n",
        "    cmd = [sys.executable, '-m', 'pip'] + args\n",
        "    print('>>>', ' '.join(cmd))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "# Install the package in editable mode from the cloned repo\n",
        "os.chdir('/content/cosmos-predict2.5')\n",
        "\n",
        "# Install core dependencies first\n",
        "pip(['install', '--upgrade', \n",
        "     'transformers', 'accelerate', 'safetensors', 'huggingface_hub'])\n",
        "\n",
        "# Install the package (this will install from pyproject.toml)\n",
        "pip(['install', '-e', '.'])\n",
        "\n",
        "# Install additional runtime dependencies\n",
        "pip(['install', \n",
        "     'decord', 'einops', 'imageio[ffmpeg]', \n",
        "     'opencv-python-headless', 'pillow', 'numpy'])\n",
        "\n",
        "print(\"‚úÖ Cosmos-Predict 2.5 installation complete\")\n",
        "PY\n",
        "\n",
        "echo \"\"\n",
        "echo \"üîé Verifying installation\"\n",
        "/usr/local/bin/micromamba run -n cosmos310 python - <<'PY'\n",
        "import torch\n",
        "print('='*60)\n",
        "print('Python:', sys.version.split()[0])\n",
        "print('Torch:', torch.__version__)\n",
        "print('CUDA:', torch.version.cuda)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "    cc_major, cc_minor = torch.cuda.get_device_capability(0)\n",
        "    print(f'Compute Capability: {cc_major}.{cc_minor}')\n",
        "    print('BF16 supported (Ampere+ expected):', cc_major >= 8)\n",
        "    \n",
        "    # Check driver\n",
        "    import subprocess\n",
        "    try:\n",
        "        result = subprocess.run(['nvidia-smi', '--query-gpu=driver_version', '--format=csv,noheader'], \n",
        "                              capture_output=True, text=True)\n",
        "        driver = result.stdout.strip()\n",
        "        print(f'NVIDIA Driver: {driver}')\n",
        "        driver_num = float(driver.split('.')[0])\n",
        "        if driver_num >= 570:\n",
        "            print('‚úÖ Driver version compatible with CUDA 12.8.1')\n",
        "        else:\n",
        "            print(f'‚ö†Ô∏è Driver {driver} may not support CUDA 12.8.1 (requires >=570.124.06)')\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "print('='*60)\n",
        "PY\n",
        "\n",
        "echo \"\"\n",
        "echo \"‚úÖ Installation complete!\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Verify Cosmos‚ÄëPredict 2.5 import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "export MAMBA_ROOT_PREFIX=/content/micromamba\n",
        "/usr/local/bin/micromamba run -n cosmos310 python - <<'PY'\n",
        "try:\n",
        "    import cosmos_predict2\n",
        "    from cosmos_predict2 import inference\n",
        "    print('‚úÖ cosmos_predict2 imported')\n",
        "    # Show available helpers if present\n",
        "    print('inference attrs:', [a for a in dir(inference) if 'Video' in a or 'World' in a][:8])\n",
        "except Exception as e:\n",
        "    print('‚ùå Import failed:', e)\n",
        "PY"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Helpers to run code inside the **cosmos310** env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess, tempfile, os, json, glob, shutil\n",
        "\n",
        "def run_cosmos_py(code, echo=True):\n",
        "    with tempfile.NamedTemporaryFile('w', suffix='.py', delete=False) as f:\n",
        "        f.write(code)\n",
        "        path = f.name\n",
        "    try:\n",
        "        env = os.environ.copy(); env['MAMBA_ROOT_PREFIX'] = '/content/micromamba'\n",
        "        cmd = ['/usr/local/bin/micromamba','run','-n','cosmos310','python',path]\n",
        "        res = subprocess.run(cmd, text=True, capture_output=True, env=env)\n",
        "        if echo:\n",
        "            print(res.stdout)\n",
        "            if res.returncode != 0:\n",
        "                print(res.stderr)\n",
        "        return res\n",
        "    finally:\n",
        "        try: os.unlink(path)\n",
        "        except: pass\n",
        "\n",
        "def run_cosmos_cmd(cmd):\n",
        "    full = f\"export MAMBA_ROOT_PREFIX=/content/micromamba && /usr/local/bin/micromamba run -n cosmos310 {cmd}\"\n",
        "    return subprocess.run(full, shell=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: **Download Model Checkpoints** (2 options)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: **Download Model Checkpoints**\n",
        "\n",
        "According to the official docs, checkpoints are automatically downloaded during inference and post-training to the Hugging Face cache (controlled by `HF_HOME` environment variable).\n",
        "\n",
        "You can also pre-download using the checkpoint downloader script:\n",
        "\n",
        "```bash\n",
        "# For 2B Video2World model (default 720p/16fps)\n",
        "python -m scripts.download_checkpoints --model_types video2world --model_sizes 2B\n",
        "\n",
        "# For lower VRAM (24GB GPUs): 480p/10fps variant\n",
        "python -m scripts.download_checkpoints --model_types video2world --model_sizes 2B --resolution 480 --fps 10\n",
        "```\n",
        "\n",
        "Alternatively, checkpoints will auto-download on first inference run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "export MAMBA_ROOT_PREFIX=/content/micromamba\n",
        "\n",
        "echo \"\udce6 Setting HF_HOME for checkpoint cache\"\n",
        "export HF_HOME=/content/hf_cache\n",
        "mkdir -p $HF_HOME\n",
        "\n",
        "echo \"‚ÑπÔ∏è Checkpoints will be automatically downloaded during inference\"\n",
        "echo \"Cache location: $HF_HOME\"\n",
        "echo \"\"\n",
        "echo \"Optional: Pre-download checkpoints now\"\n",
        "echo \"Running download script for 2B Video2World (720p/16fps)...\"\n",
        "\n",
        "/usr/local/bin/micromamba run -n cosmos310 bash -c \"\n",
        "export HF_HOME=$HF_HOME\n",
        "cd /content/cosmos-predict2.5\n",
        "python -m scripts.download_checkpoints --model_types video2world --model_sizes 2B 2>&1 || echo '‚ö†Ô∏è Auto-download will occur during first inference'\n",
        "\"\n",
        "\n",
        "echo \"\"\n",
        "echo \"‚úÖ Checkpoint setup complete\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Generate a Test Video (CLI example, BF16)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "export MAMBA_ROOT_PREFIX=/content/micromamba\n",
        "export HF_HOME=/content/hf_cache\n",
        "\n",
        "echo \"üñºÔ∏è Creating a simple test input image\"\n",
        "python - <<'PY'\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "# Create 720p image (1280x720)\n",
        "img = np.ones((720,1280,3), dtype=np.uint8)*96\n",
        "# Add a colored rectangle\n",
        "img[220:500, 420:860] = [210,150,100]\n",
        "Image.fromarray(img).save('/content/input0.jpg')\n",
        "print('‚úÖ Created /content/input0.jpg')\n",
        "PY\n",
        "\n",
        "echo \"\"\n",
        "echo \"üé¨ Running Video2World inference (2B model, BF16)\"\n",
        "echo \"Note: First run will auto-download checkpoints (~several GB)\"\n",
        "echo \"\"\n",
        "\n",
        "/usr/local/bin/micromamba run -n cosmos310 bash -c \"\n",
        "export HF_HOME=$HF_HOME\n",
        "cd /content/cosmos-predict2.5\n",
        "python -m examples.video2world \\\n",
        "    --model_size 2B \\\n",
        "    --input_path /content/input0.jpg \\\n",
        "    --num_conditional_frames 1 \\\n",
        "    --prompt 'A robotic arm moves smoothly across the table, picking up objects' \\\n",
        "    --save_path /content/cosmos_output.mp4 \\\n",
        "    --use_bf16\n",
        "\"\n",
        "\n",
        "if [ -f /content/cosmos_output.mp4 ]; then\n",
        "    echo \"\"\n",
        "    echo \"‚úÖ Video generation successful: /content/cosmos_output.mp4\"\n",
        "else\n",
        "    echo \"\"\n",
        "    echo \"‚ö†Ô∏è CLI inference failed. Will try direct pipeline approach below.\"\n",
        "fi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plan B: Direct pipeline call (kept close to your original). Uses BF16 autocast."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gen_py = r'''\n",
        "import os, sys\n",
        "os.environ['HF_HOME'] = '/content/hf_cache'\n",
        "sys.path.insert(0, '/content/cosmos-predict2.5')\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from einops import rearrange\n",
        "import imageio\n",
        "\n",
        "print(\"Loading Cosmos-Predict 2.5 pipeline...\")\n",
        "from cosmos_predict2.inference import Video2WorldPipeline, get_cosmos_predict2_video2world_pipeline\n",
        "\n",
        "# Get default configuration for 2B model\n",
        "config = get_cosmos_predict2_video2world_pipeline(model_size='2B')\n",
        "\n",
        "print(\"Initializing pipeline on CUDA with BF16...\")\n",
        "pipe = Video2WorldPipeline.from_config(config).to('cuda').eval()\n",
        "\n",
        "# Prepare input\n",
        "if not os.path.exists('/content/input0.jpg'):\n",
        "    print(\"Creating test input image...\")\n",
        "    img = np.ones((720,1280,3), dtype=np.uint8)*96\n",
        "    img[220:500, 420:860] = [210,150,100]\n",
        "    Image.fromarray(img).save('/content/input0.jpg')\n",
        "\n",
        "print(\"Loading input image...\")\n",
        "img = Image.open('/content/input0.jpg')\n",
        "frames = np.array(img)[None,...]  # Add time dimension\n",
        "frames = torch.from_numpy(frames).float() / 255.0\n",
        "frames = rearrange(frames, 't h w c -> 1 c t h w').to('cuda')\n",
        "\n",
        "prompt = 'A robotic arm moves smoothly across the table, picking up objects'\n",
        "num_frames = 16\n",
        "fps = 16\n",
        "seed = 42\n",
        "\n",
        "print(f\"Generating {num_frames} frames at {fps} FPS...\")\n",
        "print(f\"Prompt: {prompt}\")\n",
        "\n",
        "with torch.inference_mode():\n",
        "    with torch.autocast('cuda', dtype=torch.bfloat16):\n",
        "        out = pipe(\n",
        "            frames, \n",
        "            prompt=prompt, \n",
        "            num_frames=num_frames, \n",
        "            fps=fps, \n",
        "            seed=seed\n",
        "        )\n",
        "\n",
        "print(\"Processing output...\")\n",
        "video = out if isinstance(out, np.ndarray) else out.detach().cpu().numpy()\n",
        "\n",
        "# Handle different output formats\n",
        "if video.ndim == 5:  # (batch, channels, frames, height, width)\n",
        "    video = video[0]\n",
        "if video.shape[0] == 3:  # (channels, frames, height, width)\n",
        "    video = np.transpose(video, (1,2,3,0))  # -> (frames, height, width, channels)\n",
        "\n",
        "# Normalize to uint8\n",
        "if video.max() <= 1.0:\n",
        "    video = (video * 255).astype(np.uint8)\n",
        "else:\n",
        "    video = video.astype(np.uint8)\n",
        "\n",
        "print(f\"Writing video: {video.shape}\")\n",
        "writer = imageio.get_writer('/content/cosmos_output_direct.mp4', fps=fps)\n",
        "for frame in video:\n",
        "    writer.append_data(frame)\n",
        "writer.close()\n",
        "\n",
        "print('‚úÖ Video saved: /content/cosmos_output_direct.mp4')\n",
        "'''\n",
        "\n",
        "run_cosmos_py(gen_py)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Display the Generated Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, base64\n",
        "from IPython.display import HTML, Image, display\n",
        "\n",
        "print(\"üìä Results:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Show input\n",
        "if os.path.exists('/content/input0.jpg'):\n",
        "    print('\\nüì∏ Input Image:')\n",
        "    display(Image('/content/input0.jpg', width=640))\n",
        "\n",
        "# Find and display output video\n",
        "video_paths = [\n",
        "    '/content/cosmos_output.mp4',\n",
        "    '/content/cosmos_output_direct.mp4',\n",
        "    '/content/cosmos_output_bf16.mp4'\n",
        "]\n",
        "\n",
        "found_videos = [p for p in video_paths if os.path.exists(p)]\n",
        "\n",
        "if found_videos:\n",
        "    for video_path in found_videos:\n",
        "        file_size = os.path.getsize(video_path) / (1024 * 1024)  # MB\n",
        "        print(f'\\nüé• Generated Video: {os.path.basename(video_path)} ({file_size:.2f} MB)')\n",
        "        \n",
        "        # Display video\n",
        "        with open(video_path, 'rb') as f:\n",
        "            video_data = f.read()\n",
        "        encoded = base64.b64encode(video_data).decode('ascii')\n",
        "        display(HTML(f\"\"\"\n",
        "            <video width='640' height='360' controls autoplay loop>\n",
        "                <source src='data:video/mp4;base64,{encoded}' type='video/mp4'>\n",
        "            </video>\n",
        "        \"\"\"))\n",
        "    \n",
        "    print('\\n' + '='*60)\n",
        "    print('üéâ SUCCESS! Video generation complete.')\n",
        "    print('='*60)\n",
        "else:\n",
        "    print('\\n‚ùå No output video found.')\n",
        "    print('Check the logs above for errors (OOM, missing checkpoints, driver issues).')\n",
        "    print('\\nTroubleshooting:')\n",
        "    print('- Ensure GPU has enough VRAM (~32.5 GB for 720p/16fps)')\n",
        "    print('- Check NVIDIA driver >=570.124.06 for CUDA 12.8.1')\n",
        "    print('- Try lower resolution: --resolution 480 --fps 10')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 11: Save Results to Google Drive (if mounted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "\n",
        "out_dir = os.environ.get('DRIVE_OUTPUT_DIR')\n",
        "if out_dir and os.path.exists(out_dir):\n",
        "    print(\"‚òÅÔ∏è Backing up to Google Drive...\")\n",
        "    print(\"=\"*60)\n",
        "    \n",
        "    files_to_backup = [\n",
        "        '/content/input0.jpg',\n",
        "        '/content/cosmos_output.mp4',\n",
        "        '/content/cosmos_output_direct.mp4',\n",
        "        '/content/cosmos_output_bf16.mp4'\n",
        "    ]\n",
        "    \n",
        "    backed_up = []\n",
        "    for src_path in files_to_backup:\n",
        "        if os.path.exists(src_path):\n",
        "            filename = os.path.basename(src_path)\n",
        "            dst_path = os.path.join(out_dir, filename)\n",
        "            shutil.copy2(src_path, dst_path)\n",
        "            file_size = os.path.getsize(dst_path) / (1024 * 1024)\n",
        "            backed_up.append(f\"  ‚úÖ {filename} ({file_size:.2f} MB)\")\n",
        "    \n",
        "    if backed_up:\n",
        "        print('\\n'.join(backed_up))\n",
        "        print(\"=\"*60)\n",
        "        print(f\"üìÅ All files saved to: {out_dir}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è No files found to backup\")\n",
        "else:\n",
        "    print('‚ÑπÔ∏è Google Drive not mounted - outputs are local only')\n",
        "    print('Files will be lost when runtime disconnects')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Notes, Tips & Troubleshooting\n",
        "\n",
        "### System Requirements (from official docs)\n",
        "- **GPU**: NVIDIA Ampere architecture or newer (RTX 30 Series, A100, H100, etc.)\n",
        "- **Driver**: >=570.124.06 (compatible with CUDA 12.8.1)\n",
        "- **Python**: 3.10 (required)\n",
        "- **OS**: Linux x86-64 with glibc>=2.31\n",
        "\n",
        "### VRAM Requirements\n",
        "- **2B Model @ 720p/16fps**: ~32.5 GB VRAM (A100 recommended)\n",
        "- **2B Model @ 480p/10fps**: ~24 GB VRAM (RTX 3090/4090 class)\n",
        "- Use `--resolution 480 --fps 10` in the download script for lower VRAM\n",
        "\n",
        "### Common Issues\n",
        "\n",
        "**Out of Memory (OOM) errors:**\n",
        "- Use 2B model instead of 14B\n",
        "- Lower resolution/fps: `--resolution 480 --fps 10`\n",
        "- Ensure no other GPU processes are running\n",
        "\n",
        "**CUDA driver version insufficient:**\n",
        "- Update NVIDIA drivers to latest version >=570.124.06\n",
        "- Check with: `nvidia-smi | grep \"CUDA Version:\"`\n",
        "\n",
        "**Import errors:**\n",
        "- Verify Python 3.10: `python --version`\n",
        "- Reinstall in cosmos310 environment\n",
        "- Check all dependencies installed correctly\n",
        "\n",
        "**Checkpoint download issues:**\n",
        "- Set `HF_HOME` environment variable for custom cache location\n",
        "- Checkpoints auto-download during first inference\n",
        "- Requires Hugging Face access for some models\n",
        "\n",
        "### Performance Tips\n",
        "- Always use `--use_bf16` flag for BF16 precision (Ampere+ required)\n",
        "- BF16 is faster and uses less VRAM than FP32\n",
        "- First run will be slower due to checkpoint downloads\n",
        "\n",
        "For more help, see [GitHub Issues](https://github.com/nvidia-cosmos/cosmos-predict2.5/issues)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
