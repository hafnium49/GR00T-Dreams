{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cosmos-Predict **2.5** on Google Colab with Micromamba (Python 3.10)\n",
        "\n",
        "This notebook sets up a **Python 3.10** environment and runs **Cosmos‚ÄëPredict 2.5**.\n",
        "\n",
        "**What changed from 2.0 ‚Üí 2.5**\n",
        "- Targets **Ampere/Hopper/Blackwell** GPUs; Volta/Turing are not supported. Use **BF16** for inference.\n",
        "- 2B 720p/16fps Video2World typically needs ~**32.5 GB VRAM**; use **480p/10fps** on ~24GB cards.\n",
        "\n",
        "**Why Micromamba?**\n",
        "- Fast, clean Conda-compatible envs\n",
        "- Lets us run **Python 3.10** alongside Colab‚Äôs default\n",
        "- Easy to pin CUDA/Torch wheels\n",
        "\n",
        "**Requirements**\n",
        "- Colab **GPU** runtime (**A100/H100/H200** recommended). L4/Ada may work but is not officially listed.\n",
        "- ~5‚Äì10 minutes for initial setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": ["## Step 1: Check Current Environment"]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import sys, os, platform\n",
        "print(\"üîç Current Colab Environment:\\n\" + \"=\"*60)\n",
        "print(f\"Python: {sys.version}\")\n",
        "print(f\"Executable: {sys.executable}\")\n",
        "print(f\"Platform: {platform.platform()}\")\n",
        "\n",
        "# GPU probe\n",
        "try:\n",
        "    import torch\n",
        "    gpu_ok = torch.cuda.is_available()\n",
        "except Exception:\n",
        "    gpu_ok = False\n",
        "\n",
        "if gpu_ok:\n",
        "    name = torch.cuda.get_device_name(0)\n",
        "    props = torch.cuda.get_device_properties(0)\n",
        "    vram_gb = props.total_memory / 1024**3\n",
        "    print(f\"\\nGPU: {name}\")\n",
        "    print(f\"VRAM: {vram_gb:.1f} GB\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No GPU detected ‚Äî enable GPU in Runtime ‚Üí Change runtime type ‚Üí T4/V100/A100‚Ä¶ (A100+ recommended)\")\n",
        "\n",
        "print(\"=\"*60)\n",
        "print(\"\\nWe will create a **Python 3.10** env with Micromamba and install Torch (cu126) + Cosmos‚ÄëPredict 2.5.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## Step 2: (Optional) Mount Google Drive for Autosave",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "from google.colab import drive\nfrom datetime import datetime\nimport os, sys\n\nMOUNT_DRIVE = True\nif MOUNT_DRIVE:\n    try:\n        drive.mount('/content/drive')\n        ts = datetime.now().strftime('%Y%m%d_%H%M%S')\n        out_dir = f\"/content/drive/MyDrive/cosmos_outputs_{ts}\"\n        os.makedirs(out_dir, exist_ok=True)\n        os.environ['DRIVE_OUTPUT_DIR'] = out_dir\n        print(f\"‚úÖ Drive mounted | Session dir: {out_dir}\")\n        with open(f\"{out_dir}/session.txt\", 'w') as f:\n            f.write(f\"Session: {datetime.now()}\\n\")\n            f.write(f\"Colab Python: {sys.version}\\n\")\n    except Exception as e:\n        print(f\"‚ö†Ô∏è Could not mount Drive: {e}\\nFiles will be local only.\")\nelse:\n    print(\"‚è≠Ô∏è Skipping Drive mount ‚Äî outputs will be ephemeral.\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 3: Install Micromamba",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "echo \"üì¶ Installing Micromamba...\"\n",
        "curl -Ls https://micro.mamba.pm/api/micromamba/linux-64/latest | tar -xvj bin/micromamba -O > /usr/local/bin/micromamba 2>/dev/null\n",
        "chmod +x /usr/local/bin/micromamba\n",
        "export MAMBA_ROOT_PREFIX=/content/micromamba\n",
        "mkdir -p \"$MAMBA_ROOT_PREFIX\"\n",
        "/usr/local/bin/micromamba --version\n",
        "echo \"‚úÖ Micromamba installed\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## Step 4: Create a **Python 3.10** Environment",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "export MAMBA_ROOT_PREFIX=/content/micromamba\n",
        "echo \"üêç Creating env: cosmos310 (Python 3.10)\"\n",
        "/usr/local/bin/micromamba create -y -n cosmos310 python=3.10 pip -c conda-forge\n",
        "/usr/local/bin/micromamba run -n cosmos310 python --version\n",
        "echo \"‚úÖ Env created\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## Step 5: Install **PyTorch (cu126)** and **Cosmos‚ÄëPredict 2.5**",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "export MAMBA_ROOT_PREFIX=/content/micromamba\n",
        "echo \"üì¶ Installing PyTorch cu126 (Torch 2.6 line)\"\n",
        "/usr/local/bin/micromamba run -n cosmos310 python - <<'PY'\n",
        "import sys, subprocess\n",
        "def pip(args):\n",
        "    cmd = [sys.executable, '-m', 'pip'] + args\n",
        "    print('>>>', ' '.join(cmd))\n",
        "    subprocess.check_call(cmd)\n",
        "\n",
        "# Torch cu126 wheels + matching torchvision/torchaudio\n",
        "pip([\n",
        "  'install', '--upgrade',\n",
        "  '--index-url', 'https://download.pytorch.org/whl/cu126',\n",
        "  'torch==2.6.0', 'torchvision==0.21.0', 'torchaudio==2.6.0'\n",
        "])\n",
        "\n",
        "# Faster path with NVIDIA prebuilt deps (flash-attn, TE, natten) if available\n",
        "USE_PREBUILT='1'\n",
        "cosmos_pkg = 'cosmos-predict2[cu126]>=2.5,<2.6'\n",
        "if USE_PREBUILT == '1':\n",
        "    pip(['install', cosmos_pkg,\n",
        "         '--extra-index-url', 'https://nvidia-cosmos.github.io/cosmos-dependencies/cu126_torch260/simple'])\n",
        "else:\n",
        "    pip(['install', 'cosmos-predict2>=2.5,<2.6'])\n",
        "\n",
        "# Quality-of-life deps\n",
        "pip(['install', 'transformers', 'accelerate', 'decord', 'einops', 'imageio[ffmpeg]', 'opencv-python-headless', 'pillow', 'huggingface_hub'])\n",
        "print(\"‚úÖ Installs complete\")\n",
        "PY\n",
        "\n",
        "echo \"\\nüîé Torch & CUDA check\"\n",
        "/usr/local/bin/micromamba run -n cosmos310 python - <<'PY'\n",
        "import torch\n",
        "print('Torch:', torch.__version__)\n",
        "print('CUDA:', torch.version.cuda)\n",
        "print('CUDA available:', torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print('GPU:', torch.cuda.get_device_name(0))\n",
        "    # BF16 capability probe\n",
        "    cc_major = torch.cuda.get_device_capability(0)[0]\n",
        "    print('BF16 supported (Ampere+ expected):', cc_major >= 8)\n",
        "PY"
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## Step 6: Verify Cosmos‚ÄëPredict 2.5 import",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%bash\n",
        "export MAMBA_ROOT_PREFIX=/content/micromamba\n",
        "/usr/local/bin/micromamba run -n cosmos310 python - <<'PY'\n",
        "try:\n",
        "    import cosmos_predict2\n",
        "    from cosmos_predict2 import inference\n",
        "    print('‚úÖ cosmos_predict2 imported')\n",
        "    # Show available helpers if present\n",
        "    print('inference attrs:', [a for a in dir(inference) if 'Video' in a or 'World' in a][:8])\n",
        "except Exception as e:\n",
        "    print('‚ùå Import failed:', e)\n",
        "PY"
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## Step 7: Helpers to run code inside the **cosmos310** env",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import subprocess, tempfile, os, json, glob, shutil\n",
        "\n",
        "def run_cosmos_py(code, echo=True):\n",
        "    with tempfile.NamedTemporaryFile('w', suffix='.py', delete=False) as f:\n",
        "        f.write(code)\n",
        "        path = f.name\n", 
        "    try:\n",
        "        env = os.environ.copy(); env['MAMBA_ROOT_PREFIX'] = '/content/micromamba'\n",
        "        cmd = ['/usr/local/bin/micromamba','run','-n','cosmos310','python',path]\n",
        "        res = subprocess.run(cmd, text=True, capture_output=True, env=env)\n",
        "        if echo:\n",
        "            print(res.stdout)\n",
        "            if res.returncode != 0:\n",
        "                print(res.stderr)\n",
        "        return res\n",
        "    finally:\n",
        "        try: os.unlink(path)\n",
        "        except: pass\n",
        "\n",
        "def run_cosmos_cmd(cmd):\n",
        "    full = f\"export MAMBA_ROOT_PREFIX=/content/micromamba && /usr/local/bin/micromamba run -n cosmos310 {cmd}\"\n",
        "    return subprocess.run(full, shell=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## Step 8: **Download Model Checkpoints** (2 options)",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Option A (recommended): repo downloader.** Works with multiple resolutions/fps.\n",
        "\n",
        "```bash\n",
        "python -m scripts.download_checkpoints --model_types video2world --model_sizes 2B            # 720p/16fps by default\n",
        "python -m scripts.download_checkpoints --model_types video2world --model_sizes 2B --resolution 480 --fps 10   # 24GB GPUs\n",
        "```\n",
        "\n",
        "**Option B (fallback): Hugging Face snapshot (Cosmos‚ÄëPredict2.5‚Äë2B)**"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "export MAMBA_ROOT_PREFIX=/content/micromamba\n",
        "echo \"üì• Cloning cosmos-predict2.5 repo (for downloader/scripts)\"\n",
        "git clone --depth 1 https://github.com/nvidia-cosmos/cosmos-predict2.5 /content/cosmos-predict2.5 || true\n",
        "\n",
        "echo \"üîß Installing repo (editable) to expose scripts/*\"\n",
        "/usr/local/bin/micromamba run -n cosmos310 python -m pip install -e /content/cosmos-predict2.5\n",
        "\n",
        "echo \"üì¶ Downloading 2B Video2World (default 720p/16fps). For 24GB GPUs, rerun with --resolution 480 --fps 10\"\n",
        "/usr/local/bin/micromamba run -n cosmos310 python -m scripts.download_checkpoints --model_types video2world --model_sizes 2B --checkpoint_dir /content/cosmos_ckpts || true\n",
        "echo \"‚úÖ Download step attempted (see logs above).\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Fallback snapshot (if the script didn't fetch). You must have HF access for the model.\n",
        "fallback_code = r'''\n",
        "from huggingface_hub import snapshot_download\n",
        "import os\n",
        "dst = '/content/cosmos_ckpts_fallback'\n",
        "os.makedirs(dst, exist_ok=True)\n",
        "repo_id = 'nvidia/Cosmos-Predict2.5-2B'\n",
        "path = snapshot_download(repo_id=repo_id, cache_dir=dst, resume_download=True)\n",
        "print('‚úÖ HF snapshot at:', path)\n",
        "open('/content/cosmos_ckpt_path.txt','w').write(path)\n",
        "'''\n",
        "run_cosmos_py(fallback_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## Step 9: Generate a Test Video (CLI example, BF16)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "export MAMBA_ROOT_PREFIX=/content/micromamba\n",
        "echo \"üñºÔ∏è Creating a simple input image\"\n",
        "python - <<'PY'\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "img = np.ones((704,1280,3), dtype=np.uint8)*96\n",
        "img[220:500, 420:860] = [210,150,100]\n",
        "Image.fromarray(img).save('/content/input0.jpg')\n",
        "print('saved /content/input0.jpg')\n",
        "PY\n",
        "\n",
        "echo \"üé¨ Running examples.video2world (2B, 720p/16fps). If you OOM, download 480p/10fps and add --resolution 480 --fps 10.\"\n",
        "/usr/local/bin/micromamba run -n cosmos310 python - <<'PY'\n",
        "import os, subprocess, sys\n",
        "cmd = [sys.executable, '-m', 'examples.video2world',\n",
        "       '--model_size','2B',\n",
        "       '--input_path','/content/input0.jpg',\n",
        "       '--num_conditional_frames','1',\n",
        "       '--prompt','A robotic arm moves smoothly across the table, picking up objects',\n",
        "       '--save_path','/content/cosmos_output.mp4',\n",
        "       '--use_bf16']\n",
        "print('>>>', ' '.join(cmd))\n",
        "try:\n",
        "    subprocess.check_call(cmd)\n",
        "    print('‚úÖ Video saved: /content/cosmos_output.mp4')\n",
        "except subprocess.CalledProcessError as e:\n",
        "    print('‚ùå CLI failed (likely missing checkpoint variant). Will try direct pipeline next.')\n",
        "PY"
      ]
    },
    {
      "cell_type": "markdown",
      "source": "### Plan B: Direct pipeline call (kept close to your original). Uses BF16 autocast.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "gen_py = r'''\n",
        "import os, glob, numpy as np, torch\n",
        "from PIL import Image\n",
        "from einops import rearrange\n",
        "from cosmos_predict2.inference import Video2WorldPipeline, get_cosmos_predict2_video2world_pipeline\n",
        "import imageio\n",
        "\n",
        "ckpt_dir = '/content/cosmos_ckpts' if os.path.exists('/content/cosmos_ckpts') else None\n",
        "if not ckpt_dir and os.path.exists('/content/cosmos_ckpts_fallback'):\n",
        "    ckpt_dir = open('/content/cosmos_ckpt_path.txt').read().strip()\n",
        "print('Using checkpoint root:', ckpt_dir)\n",
        "\n",
        "config = get_cosmos_predict2_video2world_pipeline(model_size='2B')\n",
        "if ckpt_dir:\n",
        "    cands = glob.glob(os.path.join(ckpt_dir,'**','*.pt'), recursive=True) or \\\n",
        "            glob.glob(os.path.join(ckpt_dir,'**','*.safetensors'), recursive=True)\n",
        "    if cands:\n",
        "        config['dit_checkpoint_path'] = cands[0]\n",
        "        print('Model checkpoint:', os.path.basename(cands[0]))\n",
        "\n",
        "pipe = Video2WorldPipeline.from_config(config).to('cuda').eval()\n",
        "\n",
        "# Read/create input\n",
        "if not os.path.exists('/content/input0.jpg'):\n",
        "    from PIL import Image\n",
        "    img = np.ones((704,1280,3), dtype=np.uint8)*96\n",
        "    Image.fromarray(img).save('/content/input0.jpg')\n",
        "img = Image.open('/content/input0.jpg')\n",
        "frames = np.array(img)[None,...]\n",
        "frames = torch.from_numpy(frames).float()/255.0\n",
        "frames = rearrange(frames, 't h w c -> 1 c t h w').to('cuda')\n",
        "\n",
        "prompt = 'A robotic arm moves smoothly across the table, picking up objects'\n",
        "num_frames = 16\n",
        "\n",
        "with torch.inference_mode():\n",
        "    with torch.autocast('cuda', dtype=torch.bfloat16):\n",
        "        out = pipe(frames, prompt=prompt, num_frames=num_frames, fps=16, seed=42)\n",
        "\n",
        "video = out if isinstance(out, np.ndarray) else out.detach().cpu().numpy()\n",
        "if video.ndim == 5: video = video[0]\n",
        "if video.shape[0] == 3: video = np.transpose(video, (1,2,3,0))\n",
        "if video.max() <= 1.0: video = (video*255).astype(np.uint8)\n",
        "writer = imageio.get_writer('/content/cosmos_output_bf16.mp4', fps=16)\n",
        "for f in video: writer.append_data(f)\n",
        "writer.close()\n",
        "print('‚úÖ Saved /content/cosmos_output_bf16.mp4')\n",
        "'''\n",
        "run_cosmos_py(gen_py)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## Step 10: Display the Generated Video",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, base64\n",
        "from IPython.display import HTML, Image, display\n",
        "if os.path.exists('/content/input0.jpg'):\n",
        "    print('üì∏ Input image:')\n",
        "    display(Image('/content/input0.jpg', width=400))\n",
        "paths = [p for p in ['/content/cosmos_output.mp4','/content/cosmos_output_bf16.mp4'] if os.path.exists(p)]\n",
        "if paths:\n",
        "    p = paths[-1]\n",
        "    print('üé• Generated video:', p)\n",
        "    data = open(p,'rb').read(); enc = base64.b64encode(data).decode('ascii')\n",
        "    display(HTML(f\"<video width='640' height='360' controls autoplay loop><source src='data:video/mp4;base64,{enc}' type='video/mp4'></video>\"))\n",
        "    print('üéâ Success')\n",
        "else:\n",
        "    print('‚ùå No video found. Check logs above for OOM or missing checkpoints.')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": "## Step 11: Save Results to Google Drive (if mounted)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, shutil\n",
        "out_dir = os.environ.get('DRIVE_OUTPUT_DIR')\n",
        "if out_dir and os.path.exists(out_dir):\n",
        "    for p in ['/content/cosmos_output.mp4','/content/cosmos_output_bf16.mp4','/content/input0.jpg']:\n",
        "        if os.path.exists(p):\n",
        "            dst = os.path.join(out_dir, os.path.basename(p))\n",
        "            shutil.copy2(p, dst)\n",
        "            print('‚òÅÔ∏è Saved to Drive:', dst)\n",
        "else:\n",
        "    print('‚ÑπÔ∏è Drive not mounted; skipping backup.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Notes, Tips & Troubleshooting\n",
        "\n",
        "- **Python version**: Project scripts and docs assume **Python 3.10** (Conda examples even symlink headers under `python3.10`).\n",
        "- **GPU & precision**: 2.5 officially lists **Ampere/Hopper/Blackwell** and **BF16** for inference; 2B 720p/16fps needs ~**32.5 GB** VRAM.\n",
        "- **Lower VRAM path**: use the downloader with `--resolution 480 --fps 10` for ~24‚ÄØGB class cards.\n",
        "- **If `examples.video2world` fails**: fall back to the direct pipeline cell (Plan B), which sets BF16 autocast.\n",
        "- **If wheels conflict**: comment out the extra index line and expect source builds as described in NVIDIA‚Äôs docs (flash‚Äëattn/TE/NATTEN)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
    "language_info": { "name": "python", "version": "3.10.0" },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
